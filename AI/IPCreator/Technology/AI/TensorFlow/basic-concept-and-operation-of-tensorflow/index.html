<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/img/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/img/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/img/favicon.ico">
  <link rel="mask-icon" href="/img/apple-touch-icon.png" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"hazyman.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":true,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":true},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.json"};
  </script>

  <meta name="description" content="作者：林海山波   本文主要对tf的一些常用概念与方法进行描述。">
<meta name="keywords" content="Tensorflow">
<meta property="og:type" content="article">
<meta property="og:title" content="Basic Concept of Tensorflow">
<meta property="og:url" content="https://hazyman.com/AI/IPCreator/Technology/AI/TensorFlow/basic-concept-and-operation-of-tensorflow/index.html">
<meta property="og:site_name" content="生命之旅">
<meta property="og:description" content="作者：林海山波   本文主要对tf的一些常用概念与方法进行描述。">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2020-02-22T00:11:20.877Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Basic Concept of Tensorflow">
<meta name="twitter:description" content="作者：林海山波   本文主要对tf的一些常用概念与方法进行描述。">

<link rel="canonical" href="https://hazyman.com/AI/IPCreator/Technology/AI/TensorFlow/basic-concept-and-operation-of-tensorflow/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Basic Concept of Tensorflow | 生命之旅</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="生命之旅" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">生命之旅</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Thinker, Doer, Innovator.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://hazyman.com/AI/IPCreator/Technology/AI/TensorFlow/basic-concept-and-operation-of-tensorflow/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/content/Kick-Off.jpg">
      <meta itemprop="name" content="IPCreator">
      <meta itemprop="description" content="Life is a journey.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="生命之旅">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Basic Concept of Tensorflow
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/AI/" itemprop="url" rel="index"><span itemprop="name">AI</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/AI/IPCreator/Technology/AI/TensorFlow/basic-concept-and-operation-of-tensorflow/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/AI/IPCreator/Technology/AI/TensorFlow/basic-concept-and-operation-of-tensorflow/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><a href="http://blog.csdn.net/lenbow/article/details/52152766" target="_blank" rel="noopener">作者：林海山波</a></p>
<p><img data-src="http://avatar.csdn.net/D/1/1/1_lenbow.jpg" alt></p>
<p> 本文主要对tf的一些常用概念与方法进行描述。</p>
  <a id="more"></a>
  <h3 id="1tensorflow的基本运作">1、tensorflow的基本运作</h3>

  <p>为了快速的熟悉TensorFlow编程，下面从一段简单的代码开始：</p>



  <pre class="prettyprint"><code class="language-python hljs "><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
   <span class="hljs-comment">#定义‘符号’变量，也称为占位符</span>
   a = tf.placeholder(<span class="hljs-string">"float"</span>)
   b = tf.placeholder(<span class="hljs-string">"float"</span>)

   y = tf.mul(a, b) <span class="hljs-comment">#构造一个op节点</span>

   sess = tf.Session()<span class="hljs-comment">#建立会话</span>
   <span class="hljs-comment">#运行会话，输入数据，并计算节点，同时打印结果</span>
   <span class="hljs-keyword">print</span> sess.run(y, feed_dict={a: <span class="hljs-number">3</span>, b: <span class="hljs-number">3</span>})
   <span class="hljs-comment"># 任务完成, 关闭会话.</span>
   sess.close()</code></pre>

  <p>其中tf.mul(a, b)函数便是tf的一个基本的算数运算，接下来介绍跟多的相关函数。</p>



  <h3 id="2tf函数">2、tf函数</h3>

  <pre><code>TensorFlow 将图形定义转换成分布式执行的操作, 以充分利用可用的计算资源(如 CPU 或 GPU。一般你不需要显式指定使用 CPU 还是 GPU, TensorFlow 能自动检测。如果检测到 GPU, TensorFlow 会尽可能地利用找到的第一个 GPU 来执行操作.
  并行计算能让代价大的算法计算加速执行，TensorFlow也在实现上对复杂操作进行了有效的改进。大部分核相关的操作都是设备相关的实现，比如GPU。下面是一些重要的操作/核：
  </code></pre>

  <table><br>  <thead><br>  <tr><br>    <th>操作组</th><br>    <th align="left">操作</th><br>  </tr><br>  </thead><br>  <tbody><tr><br>    <td>Maths</td><br>    <td align="left">Add, Sub, Mul, Div, Exp, Log, Greater, Less, Equal</td><br>  </tr><br>  <tr><br>    <td>Array</td><br>    <td align="left">Concat, Slice, Split, Constant, Rank, Shape, Shuffle</td><br>  </tr><br>  <tr><br>    <td>Matrix</td><br>    <td align="left">MatMul, MatrixInverse, MatrixDeterminant</td><br>  </tr><br>  <tr><br>    <td>Neuronal Network</td><br>    <td align="left">SoftMax, Sigmoid, ReLU, Convolution2D, MaxPool</td><br>  </tr><br>  <tr><br>    <td>Checkpointing</td><br>    <td align="left">Save, Restore</td><br>  </tr><br>  <tr><br>    <td>Queues and syncronizations</td><br>    <td align="left">Enqueue, Dequeue, MutexAcquire, MutexRelease</td><br>  </tr><br>  <tr><br>    <td>Flow control</td><br>    <td align="left">Merge, Switch, Enter, Leave, NextIteration</td><br>  </tr><br>  </tbody></table>




  <h4 id="tensorflow的算术操作如下">TensorFlow的算术操作如下：</h4>

  <table><br>  <thead><br>  <tr><br>    <th>操作</th><br>    <th align="left">描述</th><br>  </tr><br>  </thead><br>  <tbody><tr><br>    <td>tf.add(x, y, name=None)</td><br>    <td align="left">求和</td><br>  </tr><br>  <tr><br>    <td>tf.sub(x, y, name=None)</td><br>    <td align="left">减法</td><br>  </tr><br>  <tr><br>    <td>tf.mul(x, y, name=None)</td><br>    <td align="left">乘法</td><br>  </tr><br>  <tr><br>    <td>tf.div(x, y, name=None)</td><br>    <td align="left">除法</td><br>  </tr><br>  <tr><br>    <td>tf.mod(x, y, name=None)</td><br>    <td align="left">取模</td><br>  </tr><br>  <tr><br>    <td>tf.abs(x, name=None)</td><br>    <td align="left">求绝对值</td><br>  </tr><br>  <tr><br>    <td>tf.neg(x, name=None)</td><br>    <td align="left">取负  (y = -x).</td><br>  </tr><br>  <tr><br>    <td>tf.sign(x, name=None)</td><br>    <td align="left">返回符号 y = sign(x) = -1 if x &lt; 0; 0 if x == 0; 1 if x &gt; 0.</td><br>  </tr><br>  <tr><br>    <td>tf.inv(x, name=None)</td><br>    <td align="left">取反</td><br>  </tr><br>  <tr><br>    <td>tf.square(x, name=None)</td><br>    <td align="left">计算平方 (y = x * x = x^2).</td><br>  </tr><br>  <tr><br>    <td>tf.round(x, name=None)</td><br>    <td align="left">舍入最接近的整数<br># ‘a’ is [0.9, 2.5, 2.3, -4.4]<br>tf.round(a) ==&gt; [ 1.0, 3.0, 2.0, -4.0 ]</td><br>  </tr><br>  <tr><br>    <td>tf.sqrt(x, name=None)</td><br>    <td align="left">开根号 (y = \sqrt{x} = x^{1/2}).</td><br>  </tr><br>  <tr><br>    <td>tf.pow(x, y, name=None)</td><br>    <td align="left">幂次方 <br># tensor ‘x’ is [[2, 2], [3, 3]]<br># tensor ‘y’ is [[8, 16], [2, 3]]<br>tf.pow(x, y) ==&gt; [[256, 65536], [9, 27]]</td><br>  </tr><br>  <tr><br>    <td>tf.exp(x, name=None)</td><br>    <td align="left">计算e的次方</td><br>  </tr><br>  <tr><br>    <td>tf.log(x, name=None)</td><br>    <td align="left">计算log，一个输入计算e的ln，两输入以第二输入为底</td><br>  </tr><br>  <tr><br>    <td>tf.maximum(x, y, name=None)</td><br>    <td align="left">返回最大值 (x &gt; y ? x : y)</td><br>  </tr><br>  <tr><br>    <td>tf.minimum(x, y, name=None)</td><br>    <td align="left">返回最小值 (x &lt; y ? x : y)</td><br>  </tr><br>  <tr><br>    <td>tf.cos(x, name=None)</td><br>    <td align="left">三角函数cosine</td><br>  </tr><br>  <tr><br>    <td>tf.sin(x, name=None)</td><br>    <td align="left">三角函数sine</td><br>  </tr><br>  <tr><br>    <td>tf.tan(x, name=None)</td><br>    <td align="left">三角函数tan</td><br>  </tr><br>  <tr><br>    <td>tf.atan(x, name=None)</td><br>    <td align="left">三角函数ctan</td><br>  </tr><br>  </tbody></table>


  <hr>



  <h4 id="张量操作tensor-transformations">张量操作Tensor Transformations</h4>

  <ul><br>  <li>数据类型转换Casting</li><br>  </ul>

  <table><br>  <thead><br>  <tr><br>    <th>操作</th><br>    <th align="left">描述</th><br>  </tr><br>  </thead><br>  <tbody><tr><br>    <td>tf.string_to_number<br>(string_tensor, out_type=None, name=None)</td><br>    <td align="left">字符串转为数字</td><br>  </tr><br>  <tr><br>    <td>tf.to_double(x, name=’ToDouble’)</td><br>    <td align="left">转为64位浮点类型–float64</td><br>  </tr><br>  <tr><br>    <td>tf.to_float(x, name=’ToFloat’)</td><br>    <td align="left">转为32位浮点类型–float32</td><br>  </tr><br>  <tr><br>    <td>tf.to_int32(x, name=’ToInt32’)</td><br>    <td align="left">转为32位整型–int32</td><br>  </tr><br>  <tr><br>    <td>tf.to_int64(x, name=’ToInt64’)</td><br>    <td align="left">转为64位整型–int64</td><br>  </tr><br>  <tr><br>    <td>tf.cast(x, dtype, name=None)</td><br>    <td align="left">将x或者x.values转换为dtype<br># tensor <code>a</code> is [1.8, 2.2], dtype=tf.float<br>tf.cast(a, tf.int32) ==&gt; [1, 2]  # dtype=tf.int32</td><br>  </tr><br>  <tr><br>    <td></td><br>    <td align="left"></td><br>  </tr><br>  </tbody></table>


  <ul><br>  <li>形状操作Shapes and Shaping</li><br>  </ul>

  <table><br>  <thead><br>  <tr><br>    <th>操作</th><br>    <th align="left">描述</th><br>  </tr><br>  </thead><br>  <tbody><tr><br>    <td>tf.shape(input, name=None)</td><br>    <td align="left">返回数据的shape<br># ‘t’ is [[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]<br>shape(t) ==&gt; [2, 2, 3]</td><br>  </tr><br>  <tr><br>    <td>tf.size(input, name=None)</td><br>    <td align="left">返回数据的元素数量<br># ‘t’ is [[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]]<br>size(t) ==&gt; 12</td><br>  </tr><br>  <tr><br>    <td>tf.rank(input, name=None)</td><br>    <td align="left">返回tensor的rank<br>注意：此rank不同于矩阵的rank，<br>tensor的rank表示一个tensor需要的索引数目来唯一表示任何一个元素<br>也就是通常所说的 “order”, “degree”或”ndims”<br>#’t’ is [[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]<br># shape of tensor ‘t’ is [2, 2, 3]<br>rank(t) ==&gt; 3</td><br>  </tr><br>  <tr><br>    <td>tf.reshape(tensor, shape, name=None)</td><br>    <td align="left">改变tensor的形状<br># tensor ‘t’ is [1, 2, 3, 4, 5, 6, 7, 8, 9]<br># tensor ‘t’ has shape [9]<br>reshape(t, [3, 3]) ==&gt; <br>[[1, 2, 3],<br>[4, 5, 6],<br>[7, 8, 9]]<br>#如果shape有元素[-1],表示在该维度打平至一维<br># -1 将自动推导得为 9:<br>reshape(t, [2, -1]) ==&gt; <br>[[1, 1, 1, 2, 2, 2, 3, 3, 3],<br>[4, 4, 4, 5, 5, 5, 6, 6, 6]]</td><br>  </tr><br>  <tr><br>    <td>tf.expand_dims(input, dim, name=None)</td><br>    <td align="left">插入维度1进入一个tensor中<br>#该操作要求-1-input.dims()<br># ‘t’ is a tensor of shape [2]<br>shape(expand_dims(t, 0)) ==&gt; [1, 2]<br>shape(expand_dims(t, 1)) ==&gt; [2, 1]<br>shape(expand_dims(t, -1)) ==&gt; [2, 1] &lt;= dim &lt;= input.dims()</td><br>  </tr><br>  </tbody></table>


  <ul><br>  <li>切片与合并（Slicing and Joining）</li><br>  </ul>

  <table><br>  <thead><br>  <tr><br>    <th>操作</th><br>    <th align="left">描述</th><br>  </tr><br>  </thead><br>  <tbody><tr><br>    <td>tf.slice(input_, begin, size, name=None)</td><br>    <td align="left">对tensor进行切片操作<br>其中size[i] = input.dim_size(i) - begin[i]<br>该操作要求 0 &lt;= begin[i] &lt;= begin[i] + size[i] &lt;= Di for i in [0, n]<br>#’input’ is <br>#[[[1, 1, 1], [2, 2, 2]],[[3, 3, 3], [4, 4, 4]],[[5, 5, 5], [6, 6, 6]]]<br>tf.slice(input, [1, 0, 0], [1, 1, 3]) ==&gt; [[[3, 3, 3]]]<br>tf.slice(input, [1, 0, 0], [1, 2, 3]) ==&gt; <br>[[[3, 3, 3],<br>[4, 4, 4]]]<br>tf.slice(input, [1, 0, 0], [2, 1, 3]) ==&gt; <br>[[[3, 3, 3]],<br>[[5, 5, 5]]]</td><br>  </tr><br>  <tr><br>    <td>tf.split(split_dim, num_split, value, name=’split’)</td><br>    <td align="left">沿着某一维度将tensor分离为num_split tensors<br># ‘value’ is a tensor with shape [5, 30]<br># Split ‘value’ into 3 tensors along dimension 1<br>split0, split1, split2 = tf.split(1, 3, value)<br>tf.shape(split0) ==&gt; [5, 10]</td><br>  </tr><br>  <tr><br>    <td>tf.concat(concat_dim, values, name=’concat’)</td><br>    <td align="left">沿着某一维度连结tensor<br>t1 = [[1, 2, 3], [4, 5, 6]]<br>t2 = [[7, 8, 9], [10, 11, 12]]<br>tf.concat(0, [t1, t2]) ==&gt; [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]<br>tf.concat(1, [t1, t2]) ==&gt; [[1, 2, 3, 7, 8, 9], [4, 5, 6, 10, 11, 12]]<br>如果想沿着tensor一新轴连结打包,那么可以：<br>tf.concat(axis, [tf.expand_dims(t, axis) for t in tensors])<br>等同于tf.pack(tensors, axis=axis)</td><br>  </tr><br>  <tr><br>    <td>tf.pack(values, axis=0, name=’pack’)</td><br>    <td align="left">将一系列rank-R的tensor打包为一个rank-(R+1)的tensor<br># ‘x’ is [1, 4],  ‘y’ is [2, 5],  ‘z’ is [3, 6]<br>pack([x, y, z]) =&gt; [[1, 4], [2, 5], [3, 6]]  <br># 沿着第一维pack<br>pack([x, y, z], axis=1) =&gt; [[1, 2, 3], [4, 5, 6]]<br>等价于tf.pack([x, y, z]) = np.asarray([x, y, z])</td><br>  </tr><br>  <tr><br>    <td>tf.reverse(tensor, dims, name=None)</td><br>    <td align="left">沿着某维度进行序列反转<br>其中dim为列表，元素为bool型，size等于rank(tensor)<br># tensor ‘t’ is <br>[[[[ 0,  1,  2,  3],<br>#[ 4,  5,  6,  7],<br><br>#[ 8,  9, 10, 11]],<br>#[[12, 13, 14, 15],<br>#[16, 17, 18, 19],<br>#[20, 21, 22, 23]]]]<br># tensor ‘t’ shape is [1, 2, 3, 4]<br># ‘dims’ is [False, False, False, True]<br>reverse(t, dims) ==&gt;<br> [[[[ 3,  2,  1,  0],<br> [ 7,  6,  5,  4],<br>[ 11, 10, 9, 8]],<br> [[15, 14, 13, 12],<br> [19, 18, 17, 16],<br> [23, 22, 21, 20]]]]</td><br>  </tr><br>  <tr><br>    <td>tf.transpose(a, perm=None, name=’transpose’)</td><br>    <td align="left">调换tensor的维度顺序<br>按照列表perm的维度排列调换tensor顺序，<br>如为定义，则perm为(n-1…0)<br># ‘x’ is [[1 2 3],[4 5 6]]<br>tf.transpose(x) ==&gt; [[1 4], [2 5],[3 6]]<br># Equivalently<br>tf.transpose(x, perm=[1, 0]) ==&gt; [[1 4],[2 5], [3 6]]</td><br>  </tr><br>  <tr><br>    <td>tf.gather(params, indices, validate_indices=None, name=None)</td><br>    <td align="left">合并索引indices所指示params中的切片<br><img data-src="http://img.blog.csdn.net/20160808174705034" alt="tf.gather" title></td><br>  </tr><br>  <tr><br>    <td>tf.one_hot<br>(indices, depth, on_value=None, off_value=None, <br>axis=None, dtype=None, name=None)</td><br>    <td align="left">indices = [0, 2, -1, 1]<br>depth = 3<br>on_value = 5.0  <br>off_value = 0.0  <br>axis = -1  <br>#Then output is [4 x 3]: <br> output = <br> [5.0 0.0 0.0]  // one_hot(0)  <br>[0.0 0.0 5.0]  // one_hot(2) <br> [0.0 0.0 0.0]  // one_hot(-1) <br> [0.0 5.0 0.0]  // one_hot(1)</td><br>  </tr><br>  </tbody></table>


  <hr>



  <h4 id="矩阵相关运算">矩阵相关运算</h4>

  <table><br>  <thead><br>  <tr><br>    <th>操作</th><br>    <th align="left">描述</th><br>  </tr><br>  </thead><br>  <tbody><tr><br>    <td>tf.diag(diagonal, name=None)</td><br>    <td align="left">返回一个给定对角值的对角tensor<br># ‘diagonal’ is [1, 2, 3, 4]<br>tf.diag(diagonal) ==&gt; <br>[[1, 0, 0, 0]<br>[0, 2, 0, 0]<br>[0, 0, 3, 0]<br>[0, 0, 0, 4]]</td><br>  </tr><br>  <tr><br>    <td>tf.diag_part(input, name=None)</td><br>    <td align="left">功能与上面相反</td><br>  </tr><br>  <tr><br>    <td>tf.trace(x, name=None)</td><br>    <td align="left">求一个2维tensor足迹，即对角值diagonal之和</td><br>  </tr><br>  <tr><br>    <td>tf.transpose(a, perm=None, name=’transpose’)</td><br>    <td align="left">调换tensor的维度顺序<br>按照列表perm的维度排列调换tensor顺序，<br>如为定义，则perm为(n-1…0)<br># ‘x’ is [[1 2 3],[4 5 6]]<br>tf.transpose(x) ==&gt; [[1 4], [2 5],[3 6]]<br># Equivalently<br>tf.transpose(x, perm=[1, 0]) ==&gt; [[1 4],[2 5], [3 6]]</td><br>  </tr><br>  <tr><br>    <td>tf.matmul(a, b, transpose_a=False, <br>transpose_b=False, a_is_sparse=False, <br>b_is_sparse=False, name=None)</td><br>    <td align="left">矩阵相乘</td><br>  </tr><br>  <tr><br>    <td>tf.matrix_determinant(input, name=None)</td><br>    <td align="left">返回方阵的行列式</td><br>  </tr><br>  <tr><br>    <td>tf.matrix_inverse(input, adjoint=None, name=None)</td><br>    <td align="left">求方阵的逆矩阵，adjoint为True时，计算输入共轭矩阵的逆矩阵</td><br>  </tr><br>  <tr><br>    <td>tf.cholesky(input, name=None)</td><br>    <td align="left">对输入方阵cholesky分解，<br>即把一个对称正定的矩阵表示成一个下三角矩阵L和其转置的乘积的分解A=LL^T</td><br>  </tr><br>  <tr><br>    <td>tf.matrix_solve(matrix, rhs, adjoint=None, name=None)</td><br>    <td align="left">求解tf.matrix_solve(matrix, rhs, adjoint=None, name=None)<br>matrix为方阵shape为[M,M],rhs的shape为[M,K]，output为[M,K]</td><br>  </tr><br>  </tbody></table>


  <hr>



  <h4 id="复数操作">复数操作</h4>

  <table><br>  <thead><br>  <tr><br>    <th>操作</th><br>    <th align="left">描述</th><br>  </tr><br>  </thead><br>  <tbody><tr><br>    <td>tf.complex(real, imag, name=None)</td><br>    <td align="left">将两实数转换为复数形式<br># tensor ‘real’ is [2.25, 3.25]<br># tensor <code>imag</code> is [4.75, 5.75]<br>tf.complex(real, imag) ==&gt; [[2.25 + 4.75j], [3.25 + 5.75j]]</td><br>  </tr><br>  <tr><br>    <td>tf.complex_abs(x, name=None)</td><br>    <td align="left">计算复数的绝对值，即长度。<br># tensor ‘x’ is [[-2.25 + 4.75j], [-3.25 + 5.75j]]<br>tf.complex_abs(x) ==&gt; [5.25594902, 6.60492229]</td><br>  </tr><br>  <tr><br>    <td>tf.conj(input, name=None)</td><br>    <td align="left">计算共轭复数</td><br>  </tr><br>  <tr><br>    <td>tf.imag(input, name=None)<br>tf.real(input, name=None)</td><br>    <td align="left">提取复数的虚部和实部</td><br>  </tr><br>  <tr><br>    <td>tf.fft(input, name=None)</td><br>    <td align="left">计算一维的离散傅里叶变换，输入数据类型为complex64</td><br>  </tr><br>  </tbody></table>


  <hr>



  <h4 id="归约计算reduction">归约计算(Reduction)</h4>

  <table><br>  <thead><br>  <tr><br>    <th>操作</th><br>    <th align="left">描述</th><br>  </tr><br>  </thead><br>  <tbody><tr><br>    <td>tf.reduce_sum(input_tensor, reduction_indices=None, <br>keep_dims=False, name=None)</td><br>    <td align="left">计算输入tensor元素的和，或者安照reduction_indices指定的轴进行求和<br># ‘x’ is [[1, 1, 1]<br>#         [1, 1, 1]]<br>tf.reduce_sum(x) ==&gt; 6<br>tf.reduce_sum(x, 0) ==&gt; [2, 2, 2]<br>tf.reduce_sum(x, 1) ==&gt; [3, 3]<br>tf.reduce_sum(x, 1, keep_dims=True) ==&gt; [[3], [3]]<br>tf.reduce_sum(x, [0, 1]) ==&gt; 6</td><br>  </tr><br>  <tr><br>    <td>tf.reduce_prod(input_tensor, <br>reduction_indices=None, <br>keep_dims=False, name=None)</td><br>    <td align="left">计算输入tensor元素的乘积，或者安照reduction_indices指定的轴进行求乘积</td><br>  </tr><br>  <tr><br>    <td>tf.reduce_min(input_tensor, <br>reduction_indices=None, <br>keep_dims=False, name=None)</td><br>    <td align="left">求tensor中最小值</td><br>  </tr><br>  <tr><br>    <td>tf.reduce_max(input_tensor, <br>reduction_indices=None, <br>keep_dims=False, name=None)</td><br>    <td align="left">求tensor中最大值</td><br>  </tr><br>  <tr><br>    <td>tf.reduce_mean(input_tensor, <br>reduction_indices=None, <br>keep_dims=False, name=None)</td><br>    <td align="left">求tensor中平均值</td><br>  </tr><br>  <tr><br>    <td>tf.reduce_all(input_tensor, <br>reduction_indices=None, <br>keep_dims=False, name=None)</td><br>    <td align="left">对tensor中各个元素求逻辑’与’<br># ‘x’ is <br># [[True,  True]<br>#         [False, False]]<br>tf.reduce_all(x) ==&gt; False<br>tf.reduce_all(x, 0) ==&gt; [False, False]<br>tf.reduce_all(x, 1) ==&gt; [True, False]</td><br>  </tr><br>  <tr><br>    <td>tf.reduce_any(input_tensor, <br>reduction_indices=None, <br>keep_dims=False, name=None)</td><br>    <td align="left">对tensor中各个元素求逻辑’或’</td><br>  </tr><br>  <tr><br>    <td>tf.accumulate_n(inputs, shape=None, <br>tensor_dtype=None, name=None)</td><br>    <td align="left">计算一系列tensor的和<br># tensor ‘a’ is [[1, 2], [3, 4]]<br># tensor <code>b</code> is [[5, 0], [0, 6]]<br>tf.accumulate_n([a, b, a]) ==&gt; [[7, 4], [6, 14]]</td><br>  </tr><br>  <tr><br>    <td>tf.cumsum(x, axis=0, exclusive=False, <br>reverse=False, name=None)</td><br>    <td align="left">求累积和<br>tf.cumsum([a, b, c]) ==&gt; [a, a + b, a + b + c]<br>tf.cumsum([a, b, c], exclusive=True) ==&gt; [0, a, a + b]<br>tf.cumsum([a, b, c], reverse=True) ==&gt; [a + b + c, b + c, c]<br>tf.cumsum([a, b, c], exclusive=True, reverse=True) ==&gt; [b + c, c, 0]</td><br>  </tr><br>  <tr><br>    <td></td><br>    <td align="left"></td><br>  </tr><br>  </tbody></table>


  <hr>



  <h4 id="分割segmentation">分割(Segmentation)</h4>

  <table><br>  <thead><br>  <tr><br>    <th>操作</th><br>    <th align="left">描述</th><br>  </tr><br>  </thead><br>  <tbody><tr><br>    <td>tf.segment_sum(data, segment_ids, name=None)</td><br>    <td align="left">根据segment_ids的分段计算各个片段的和<br>其中segment_ids为一个size与data第一维相同的tensor<br>其中id为int型数据，最大id不大于size<br>c = tf.constant([[1,2,3,4], [-1,-2,-3,-4], [5,6,7,8]])<br>tf.segment_sum(c, tf.constant([0, 0, 1]))<br>==&gt;[[0 0 0 0] <br>[5 6 7 8]]<br>上面例子分为[0,1]两id,对相同id的data相应数据进行求和,<br>并放入结果的相应id中，<br>且segment_ids只升不降</td><br>  </tr><br>  <tr><br>    <td>tf.segment_prod(data, segment_ids, name=None)</td><br>    <td align="left">根据segment_ids的分段计算各个片段的积</td><br>  </tr><br>  <tr><br>    <td>tf.segment_min(data, segment_ids, name=None)</td><br>    <td align="left">根据segment_ids的分段计算各个片段的最小值</td><br>  </tr><br>  <tr><br>    <td>tf.segment_max(data, segment_ids, name=None)</td><br>    <td align="left">根据segment_ids的分段计算各个片段的最大值</td><br>  </tr><br>  <tr><br>    <td>tf.segment_mean(data, segment_ids, name=None)</td><br>    <td align="left">根据segment_ids的分段计算各个片段的平均值</td><br>  </tr><br>  <tr><br>    <td>tf.unsorted_segment_sum(data, segment_ids,<br> num_segments, name=None)</td><br>    <td align="left">与tf.segment_sum函数类似，<br>不同在于segment_ids中id顺序可以是无序的</td><br>  </tr><br>  <tr><br>    <td>tf.sparse_segment_sum(data, indices, <br>segment_ids, name=None)</td><br>    <td align="left">输入进行稀疏分割求和<br>c = tf.constant([[1,2,3,4], [-1,-2,-3,-4], [5,6,7,8]])<br># Select two rows, one segment.<br>tf.sparse_segment_sum(c, tf.constant([0, 1]), tf.constant([0, 0])) <br> ==&gt; [[0 0 0 0]]<br>对原data的indices为[0,1]位置的进行分割，<br>并按照segment_ids的分组进行求和</td><br>  </tr><br>  </tbody></table>


  <hr>



  <h4 id="序列比较与索引提取sequence-comparison-and-indexing">序列比较与索引提取(Sequence Comparison and Indexing)</h4>

  <table><br>  <thead><br>  <tr><br>    <th>操作</th><br>    <th align="left">描述</th><br>  </tr><br>  </thead><br>  <tbody><tr><br>    <td>tf.argmin(input, dimension, name=None)</td><br>    <td align="left">返回input最小值的索引index</td><br>  </tr><br>  <tr><br>    <td>tf.argmax(input, dimension, name=None)</td><br>    <td align="left">返回input最大值的索引index</td><br>  </tr><br>  <tr><br>    <td>tf.listdiff(x, y, name=None)</td><br>    <td align="left">返回x，y中不同值的索引</td><br>  </tr><br>  <tr><br>    <td>tf.where(input, name=None)</td><br>    <td align="left">返回bool型tensor中为True的位置<br># ‘input’ tensor is <br>#[[True, False]<br>#[True, False]]<br># ‘input’ 有两个’True’,那么输出两个坐标值.<br># ‘input’的rank为2, 所以每个坐标为具有两个维度.<br>where(input) ==&gt;<br> [[0, 0],<br>[1, 0]]</td><br>  </tr><br>  <tr><br>    <td>tf.unique(x, name=None)</td><br>    <td align="left">返回一个元组tuple(y,idx)，y为x的列表的唯一化数据列表，<br>idx为x数据对应y元素的index<br># tensor ‘x’ is [1, 1, 2, 4, 4, 4, 7, 8, 8]<br>y, idx = unique(x)<br>y ==&gt; [1, 2, 4, 7, 8]<br>idx ==&gt; [0, 0, 1, 2, 2, 2, 3, 4, 4]</td><br>  </tr><br>  <tr><br>    <td>tf.invert_permutation(x, name=None)</td><br>    <td align="left">置换x数据与索引的关系<br># tensor <code>x</code> is [3, 4, 0, 2, 1]<br>invert_permutation(x) ==&gt; [2, 4, 3, 0, 1]</td><br>  </tr><br>  </tbody></table>


  <hr>



  <h4 id="神经网络neural-network">神经网络(Neural Network)</h4>

  <ul><br>  <li>激活函数（Activation Functions）</li><br>  </ul>

  <table><br>  <thead><br>  <tr><br>    <th>操作</th><br>    <th align="left">描述</th><br>  </tr><br>  </thead><br>  <tbody><tr><br>    <td>tf.nn.relu(features, name=None)</td><br>    <td align="left">整流函数：max(features, 0)</td><br>  </tr><br>  <tr><br>    <td>tf.nn.relu6(features, name=None)</td><br>    <td align="left">以6为阈值的整流函数：min(max(features, 0), 6)</td><br>  </tr><br>  <tr><br>    <td>tf.nn.elu(features, name=None)</td><br>    <td align="left">elu函数，exp(features) - 1 if &lt; 0,否则features<br><a href="http://arxiv.org/abs/1511.07289" target="_blank" rel="noopener">Exponential Linear Units (ELUs) </a></td><br>  </tr><br>  <tr><br>    <td>tf.nn.softplus(features, name=None)</td><br>    <td align="left">计算softplus：log(exp(features) + 1)</td><br>  </tr><br>  <tr><br>    <td>tf.nn.dropout(x, keep_prob, <br>noise_shape=None, seed=None, name=None)</td><br>    <td align="left">计算dropout，keep_prob为keep概率<br>noise_shape为噪声的shape</td><br>  </tr><br>  <tr><br>    <td>tf.nn.bias_add(value, bias, data_format=None, name=None)</td><br>    <td align="left">对value加一偏置量<br>此函数为tf.add的特殊情况，bias仅为一维，<br>函数通过广播机制进行与value求和,<br>数据格式可以与value不同，返回为与value相同格式</td><br>  </tr><br>  <tr><br>    <td>tf.sigmoid(x, name=None)</td><br>    <td align="left">y = 1 / (1 + exp(-x))</td><br>  </tr><br>  <tr><br>    <td>tf.tanh(x, name=None)</td><br>    <td align="left">双曲线切线激活函数</td><br>  </tr><br>  </tbody></table>


  <ul><br>  <li>卷积函数（Convolution）</li><br>  </ul>

  <table><br>  <thead><br>  <tr><br>    <th>操作</th><br>    <th align="left">描述</th><br>  </tr><br>  </thead><br>  <tbody><tr><br>    <td>tf.nn.conv2d(input, filter, strides, padding, <br>use_cudnn_on_gpu=None, data_format=None, name=None)</td><br>    <td align="left">在给定的4D input与 filter下计算2D卷积<br>输入shape为 [batch, height, width, in_channels]</td><br>  </tr><br>  <tr><br>    <td>tf.nn.conv3d(input, filter, strides, padding, name=None)</td><br>    <td align="left">在给定的5D input与 filter下计算3D卷积<br>输入shape为[batch, in_depth, in_height, in_width, in_channels]</td><br>  </tr><br>  </tbody></table>


  <ul><br>  <li>池化函数（Pooling）</li><br>  </ul>

  <table><br>  <thead><br>  <tr><br>    <th>操作</th><br>    <th align="left">描述</th><br>  </tr><br>  </thead><br>  <tbody><tr><br>    <td>tf.nn.avg_pool(value, ksize, strides, padding, <br>data_format=’NHWC’, name=None)</td><br>    <td align="left">平均方式池化</td><br>  </tr><br>  <tr><br>    <td>tf.nn.max_pool(value, ksize, strides, padding, <br>data_format=’NHWC’, name=None)</td><br>    <td align="left">最大值方法池化</td><br>  </tr><br>  <tr><br>    <td>tf.nn.max_pool_with_argmax(input, ksize, strides,<br> padding, Targmax=None, name=None)</td><br>    <td align="left">返回一个二维元组(output,argmax),最大值pooling，返回最大值及其相应的索引</td><br>  </tr><br>  <tr><br>    <td>tf.nn.avg_pool3d(input, ksize, strides, <br>padding, name=None)</td><br>    <td align="left">3D平均值pooling</td><br>  </tr><br>  <tr><br>    <td>tf.nn.max_pool3d(input, ksize, strides, <br>padding, name=None)</td><br>    <td align="left">3D最大值pooling</td><br>  </tr><br>  </tbody></table>


  <ul><br>  <li>数据标准化（Normalization）</li><br>  </ul>

  <table><br>  <thead><br>  <tr><br>    <th>操作</th><br>    <th align="left">描述</th><br>  </tr><br>  </thead><br>  <tbody><tr><br>    <td>tf.nn.l2_normalize(x, dim, epsilon=1e-12, name=None)</td><br>    <td align="left">对维度dim进行L2范式标准化<br>output = x / sqrt(max(sum(x*<em>2), epsilon))</em></td><br>  </tr><br>  <tr><br>    <td>tf.nn.sufficient_statistics(x, axes, shift=None, <br>keep_dims=False, name=None)</td><br>    <td align="left">计算与均值和方差有关的完全统计量<br>返回4维元组,元素个数，<em>元素总和，</em>元素的平方和，*shift结果<br><a href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Computing_shifted_data&amp;usg=AFQjCNG5RoY7Xvpv4xg-Wy-UJvAPh2zDQw" target="_blank" rel="noopener">参见算法介绍</a></td><br>  </tr><br>  <tr><br>    <td>tf.nn.normalize_moments(counts, mean_ss, variance_ss, shift, name=None)</td><br>    <td align="left">基于完全统计量计算均值和方差</td><br>  </tr><br>  <tr><br>    <td>tf.nn.moments(x, axes, shift=None, <br>name=None, keep_dims=False)</td><br>    <td align="left">直接计算均值与方差</td><br>  </tr><br>  </tbody></table>


  <ul><br>  <li>损失函数（Losses）</li><br>  </ul>

  <table><br>  <thead><br>  <tr><br>    <th>操作</th><br>    <th align="left">描述</th><br>  </tr><br>  </thead><br>  <tbody><tr><br>    <td>tf.nn.l2_loss(t, name=None)</td><br>    <td align="left">output = sum(t ** 2) / 2</td><br>  </tr><br>  </tbody></table>


  <ul><br>  <li>分类函数（Classification）</li><br>  </ul>

  <table><br>  <thead><br>  <tr><br>    <th>操作</th><br>    <th align="left">描述</th><br>  </tr><br>  </thead><br>  <tbody><tr><br>    <td>tf.nn.sigmoid_cross_entropy_with_logits<br>(logits, targets, name=None)*</td><br>    <td align="left">计算输入logits, targets的交叉熵</td><br>  </tr><br>  <tr><br>    <td>tf.nn.softmax(logits, name=None)</td><br>    <td align="left">计算softmax<br>softmax[i, j] = exp(logits[i, j]) / sum_j(exp(logits[i, j]))</td><br>  </tr><br>  <tr><br>    <td>tf.nn.log_softmax(logits, name=None)</td><br>    <td align="left">logsoftmax[i, j] = logits[i, j] - log(sum(exp(logits[i])))</td><br>  </tr><br>  <tr><br>    <td>tf.nn.softmax_cross_entropy_with_logits<br>(logits, labels, name=None)</td><br>    <td align="left">计算logits和labels的softmax交叉熵<br>logits, labels必须为相同的shape与数据类型</td><br>  </tr><br>  <tr><br>    <td>tf.nn.sparse_softmax_cross_entropy_with_logits<br>(logits, labels, name=None)</td><br>    <td align="left">计算logits和labels的softmax交叉熵</td><br>  </tr><br>  <tr><br>    <td>tf.nn.weighted_cross_entropy_with_logits<br>(logits, targets, pos_weight, name=None)</td><br>    <td align="left">与sigmoid_cross_entropy_with_logits()相似，<br>但给正向样本损失加了权重pos_weight</td><br>  </tr><br>  </tbody></table>


  <ul><br>  <li>符号嵌入（Embeddings）</li><br>  </ul>

  <table><br>  <thead><br>  <tr><br>    <th>操作</th><br>    <th align="left">描述</th><br>  </tr><br>  </thead><br>  <tbody><tr><br>    <td>tf.nn.embedding_lookup<br>(params, ids, partition_strategy=’mod’, <br>name=None, validate_indices=True)</td><br>    <td align="left">根据索引ids查询embedding列表params中的tensor值<br>如果len(params) &gt; 1，id将会安照partition_strategy策略进行分割<br>1、如果partition_strategy为”mod”，<br>id所分配到的位置为p = id % len(params)<br>比如有13个ids，分为5个位置，那么分配方案为：<br>[[0, 5, 10], [1, 6, 11], [2, 7, 12], [3, 8], [4, 9]]<br>2、如果partition_strategy为”div”,那么分配方案为：<br>[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10], [11, 12]]</td><br>  </tr><br>  <tr><br>    <td>tf.nn.embedding_lookup_sparse(params, <br>sp_ids, sp_weights, partition_strategy=’mod’, <br>name=None, combiner=’mean’)</td><br>    <td align="left">对给定的ids和权重查询embedding<br>1、sp_ids为一个N x M的稀疏tensor，<br>N为batch大小，M为任意，数据类型int64<br>2、sp_weights的shape与sp_ids的稀疏tensor权重，<br>浮点类型，若为None，则权重为全’1’</td><br>  </tr><br>  </tbody></table>


  <ul><br>  <li>循环神经网络（Recurrent Neural Networks）</li><br>  </ul>

  <table><br>  <thead><br>  <tr><br>    <th>操作</th><br>    <th align="left">描述</th><br>  </tr><br>  </thead><br>  <tbody><tr><br>    <td>tf.nn.rnn(cell, inputs, initial_state=None, dtype=None, <br>sequence_length=None, scope=None)</td><br>    <td align="left">基于RNNCell类的实例cell建立循环神经网络<br></td><br>  </tr><br>  <tr><br>    <td>tf.nn.dynamic_rnn(cell, inputs, sequence_length=None, <br>initial_state=None, dtype=None, parallel_iterations=None, <br>swap_memory=False, time_major=False, scope=None)</td><br>    <td align="left">基于RNNCell类的实例cell建立动态循环神经网络<br>与一般rnn不同的是，该函数会根据输入动态展开<br>返回(outputs,state)</td><br>  </tr><br>  <tr><br>    <td>tf.nn.state_saving_rnn(cell, inputs, state_saver, state_name, <br>sequence_length=None, scope=None)</td><br>    <td align="left">可储存调试状态的RNN网络</td><br>  </tr><br>  <tr><br>    <td>tf.nn.bidirectional_rnn(cell_fw, cell_bw, inputs, <br>initial_state_fw=None, initial_state_bw=None, dtype=None,<br> sequence_length=None, scope=None)</td><br>    <td align="left">双向RNN, 返回一个3元组tuple<br>(outputs, output_state_fw, output_state_bw)</td><br>  </tr><br>  </tbody></table>


  <blockquote><br>    <p>— <strong><em>tf.nn.rnn简要介绍</em></strong>— <br><br>      cell: 一个RNNCell实例 <br><br>      inputs: 一个shape为[batch_size, input_size]的tensor <br><br>      initial_state: 为RNN的state设定初值，可选 <br><br>      sequence_length：制定输入的每一个序列的长度，size为[batch_size],值范围为[0, T)的int型数据 <br><br>      其中T为输入数据序列的长度 <br><br>      @ <br><br>      @针对输入batch中序列长度不同，所设置的动态计算机制 <br><br>      @对于在时间t，和batch的b行，有 <br><br>      (output, state)(b, t) = ? (zeros(cell.output_size), states(b, sequence_length(b) - 1)) : cell(input(b, t), state(b, t - 1))</p><br><br>    <hr><br>  </blockquote>

  <ul><br>  <li>求值网络（Evaluation）</li><br>  </ul>

  <table><br>  <thead><br>  <tr><br>    <th>操作</th><br>    <th align="left">描述</th><br>  </tr><br>  </thead><br>  <tbody><tr><br>    <td>tf.nn.top_k(input, k=1, sorted=True, name=None)</td><br>    <td align="left">返回前k大的值及其对应的索引</td><br>  </tr><br>  <tr><br>    <td>tf.nn.in_top_k(predictions, targets, k, name=None)</td><br>    <td align="left">返回判断是否targets索引的predictions相应的值<br>是否在在predictions前k个位置中，<br>返回数据类型为bool类型，len与predictions同</td><br>  </tr><br>  </tbody></table>


  <ul><br>  <li><a href="https://www.tensorflow.org/versions/r0.10/extras/candidate_sampling.pdf" target="_blank" rel="noopener">监督候选采样网络（Candidate Sampling）</a></li><br>  </ul>

  <p>对于有巨大量的多分类与多标签模型，如果使用全连接softmax将会占用大量的时间与空间资源，所以采用候选采样方法仅使用一小部分类别与标签作为监督以加速训练。</p>

  <table><br>  <thead><br>  <tr><br>    <th>操作</th><br>    <th align="left">描述</th><br>  </tr><br>  </thead><br>  <tbody><tr><br>    <td><strong><em>Sampled Loss Functions</em></strong></td><br>    <td align="left"></td><br>  </tr><br>  <tr><br>    <td>tf.nn.nce_loss(weights, biases, inputs, labels, num_sampled,<br> num_classes, num_true=1, sampled_values=None,<br> remove_accidental_hits=False, partition_strategy=’mod’,<br> name=’nce_loss’)</td><br>    <td align="left">返回noise-contrastive的训练损失结果</td><br>  </tr><br>  <tr><br>    <td>tf.nn.sampled_softmax_loss(weights, biases, inputs, labels, <br>num_sampled, num_classes, num_true=1, sampled_values=None,<br> remove_accidental_hits=True, partition_strategy=’mod’, <br>name=’sampled_softmax_loss’)</td><br>    <td align="left">返回sampled softmax的训练损失<br><a href="http://arxiv.org/pdf/1412.2007.pdf" target="_blank" rel="noopener">参考- Jean et al., 2014第3部分</a></td><br>  </tr><br>  <tr><br>    <td><strong><em>Candidate Samplers</em></strong></td><br>    <td align="left"></td><br>  </tr><br>  <tr><br>    <td>tf.nn.uniform_candidate_sampler(true_classes, num_true, <br>num_sampled, unique, range_max, seed=None, name=None)</td><br>    <td align="left">通过均匀分布的采样集合<br>返回三元tuple<br>1、sampled_candidates 候选集合。<br>2、期望的true_classes个数，为浮点值<br>3、期望的sampled_candidates个数，为浮点值</td><br>  </tr><br>  <tr><br>    <td>tf.nn.log_uniform_candidate_sampler(true_classes, num_true,<br> num_sampled, unique, range_max, seed=None, name=None)</td><br>    <td align="left">通过log均匀分布的采样集合，返回三元tuple</td><br>  </tr><br>  <tr><br>    <td>tf.nn.learned_unigram_candidate_sampler<br>(true_classes, num_true, num_sampled, unique, <br>range_max, seed=None, name=None)</td><br>    <td align="left">根据在训练过程中学习到的分布状况进行采样<br>返回三元tuple</td><br>  </tr><br>  <tr><br>    <td>tf.nn.fixed_unigram_candidate_sampler(true_classes, num_true,<br> num_sampled, unique, range_max, vocab_file=”, <br>distortion=1.0, num_reserved_ids=0, num_shards=1, <br>shard=0, unigrams=(), seed=None, name=None)</td><br>    <td align="left">基于所提供的基本分布进行采样</td><br>  </tr><br>  </tbody></table>




  <h1 id="保存与恢复变量">保存与恢复变量</h1>

  <table><br>  <thead><br>  <tr><br>    <th>操作</th><br>    <th align="left">描述</th><br>  </tr><br>  </thead><br>  <tbody><tr><br>    <td>类tf.train.Saver(Saving and Restoring Variables)</td><br>    <td align="left"></td><br>  </tr><br>  <tr><br>    <td>tf.train.Saver.<strong>init</strong>(var_list=None, reshape=False, <br>sharded=False, max_to_keep=5, <br>keep_checkpoint_every_n_hours=10000.0, <br>name=None, restore_sequentially=False,<br> saver_def=None, builder=None)</td><br>    <td align="left">创建一个存储器Saver<br>var_list定义需要存储和恢复的变量</td><br>  </tr><br>  <tr><br>    <td>tf.train.Saver.save(sess, save_path, global_step=None, <br>latest_filename=None, meta_graph_suffix=’meta’,<br> write_meta_graph=True)</td><br>    <td align="left">保存变量</td><br>  </tr><br>  <tr><br>    <td>tf.train.Saver.restore(sess, save_path)</td><br>    <td align="left">恢复变量</td><br>  </tr><br>  <tr><br>    <td>tf.train.Saver.last_checkpoints</td><br>    <td align="left">列出最近未删除的checkpoint 文件名</td><br>  </tr><br>  <tr><br>    <td>tf.train.Saver.set_last_checkpoints(last_checkpoints)</td><br>    <td align="left">设置checkpoint文件名列表</td><br>  </tr><br>  <tr><br>    <td>tf.train.Saver.set_last_checkpoints_with_time(last_checkpoints_with_time)</td><br>    <td align="left">设置checkpoint文件名列表和时间戳</td><br>  </tr><br>  </tbody></table>



  <p><strong>相关链接：</strong></p>

  <p>[1] 安装Tensorflow（Linux ubuntu） <a href="http://blog.csdn.net/lenbow/article/details/51203526" target="_blank" rel="noopener">http://blog.csdn.net/lenbow/article/details/51203526</a> <br><br>[2] ubuntu下CUDA编译的GCC降级安装 <a href="http://blog.csdn.net/lenbow/article/details/51596706" target="_blank" rel="noopener">http://blog.csdn.net/lenbow/article/details/51596706</a> <br><br>[3] ubuntu手动安装最新Nvidia显卡驱动 <a href="http://blog.csdn.net/lenbow/article/details/51683783" target="_blank" rel="noopener">http://blog.csdn.net/lenbow/article/details/51683783</a> <br><br>[4] Tensorflow的CUDA升级，以及相关配置 <a href="http://blog.csdn.net/lenbow/article/details/52118116" target="_blank" rel="noopener">http://blog.csdn.net/lenbow/article/details/52118116</a> <br><br>[5] 基于gensim的Doc2Vec简析 <a href="http://blog.csdn.net/lenbow/article/details/52120230" target="_blank" rel="noopener">http://blog.csdn.net/lenbow/article/details/52120230</a> <br><br>[6] TensorFlow的分布式学习框架简介  <a href="http://blog.csdn.net/lenbow/article/details/52130565" target="_blank" rel="noopener">http://blog.csdn.net/lenbow/article/details/52130565</a></p>


<div id="article_content" class="article_content"><br>        <div class="markdown_views"><p>摘要：本文主要对tf的一些常用概念与方法进行描述。为‘Tensorflow一些常用基本概念与函数’系列之二。</p><br><br><hr><br><br><br><br><h2 id="1tensorflow的基本运作">1、tensorflow的基本运作</h2><br><br><p>为了快速的熟悉TensorFlow编程，下面从一段简单的代码开始：</p><br><br><br><br><pre class="prettyprint"><code class="language-python hljs "><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br> <span class="hljs-comment">#定义‘符号’变量，也称为占位符</span><br> a = tf.placeholder(<span class="hljs-string">“float”</span>)<br> b = tf.placeholder(<span class="hljs-string">“float”</span>)<br><br> y = tf.mul(a, b) <span class="hljs-comment">#构造一个op节点</span><br><br> sess = tf.Session()<span class="hljs-comment">#建立会话</span><br> <span class="hljs-comment">#运行会话，输入数据，并计算节点，同时打印结果</span><br> <span class="hljs-keyword">print</span> sess.run(y, feed_dict={a: <span class="hljs-number">3</span>, b: <span class="hljs-number">3</span>})<br> <span class="hljs-comment"># 任务完成, 关闭会话.</span><br> sess.close()</code></pre><br><br><p>其中tf.mul(a, b)函数便是tf的一个基本的算数运算，接下来介绍跟多的相关函数。</p><br><br><br><br><h2 id="2tf函数">2、tf函数</h2><br><br><pre><code>TensorFlow 将图形定义转换成分布式执行的操作, 以充分利用可用的计算资源(如 CPU 或 GPU。一般你不需要显式指定使用 CPU 还是 GPU, TensorFlow 能自动检测。如果检测到 GPU, TensorFlow 会尽可能地利用找到的第一个 GPU 来执行操作.<br>并行计算能让代价大的算法计算加速执行，TensorFlow也在实现上对复杂操作进行了有效的改进。大部分核相关的操作都是设备相关的实现，比如GPU。本文主要涉及的相关概念或操作有以下内容：<br></code></pre><br><br><table><br><thead><br><tr><br>  <th>操作组</th><br>  <th align="left">操作</th><br></tr><br></thead><br><tbody><tr><br>  <td>Building Graphs</td><br>  <td align="left">Core graph data structures，Tensor types，Utility functions</td><br></tr><br><tr><br>  <td>Inputs and Readers</td><br>  <td align="left">Placeholders，Readers，Converting，Queues，Input pipeline</td><br></tr><br></tbody></table><br><br><br><br><br><h3 id="21-建立图building-graphs"><strong>2.1 建立图(Building Graphs)</strong></h3><br><br><p>本节主要介绍建立tensorflow图的相关类或函数</p><br><br><h4 id="核心图的数据结构core-graph-data-structures"><em> <strong>核心图的数据结构（Core graph data structures）</strong></em></h4><br><br><p><strong><em>tf.Graph</em></strong></p><br><br><table><br><thead><br><tr><br>  <th>操作</th><br>  <th align="left">描述</th><br></tr><br></thead><br><tbody><tr><br>  <td>class tf.Graph</td><br>  <td align="left">tensorflow中的计算以图数据流的方式表示<br>一个图包含一系列表示计算单元的操作对象<br>以及在图中流动的数据单元以tensor对象表现</td><br></tr><br><tr><br>  <td>tf.Graph.<strong>init</strong>()</td><br>  <td align="left">建立一个空图</td><br></tr><br><tr><br>  <td>tf.Graph.as_default()</td><br>  <td align="left">一个将某图设置为默认图，并返回一个上下文管理器<br>如果不显式添加一个默认图，系统会自动设置一个全局的默认图。<br>所设置的默认图，在模块范围内所定义的节点都将默认加入默认图中</td><br></tr><br><tr><br>  <td>tf.Graph.as_graph_def<br>(from_version=None, add_shapes=False)</td><br>  <td align="left">返回一个图的序列化的GraphDef表示<br>序列化的<a href="https://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/core/framework/graph.proto" target="_blank" rel="noopener">GraphDef</a>可以导入至另一个图中(使用 import_graph_def())<br>或者使用C++ Session API</td><br></tr><br><tr><br>  <td>tf.Graph.finalize()</td><br>  <td align="left">完成图的构建，即将其设置为只读模式</td><br></tr><br><tr><br>  <td>tf.Graph.finalized</td><br>  <td align="left">返回True，如果图被完成</td><br></tr><br><tr><br>  <td>tf.Graph.control_dependencies(control_inputs)</td><br>  <td align="left">定义一个控制依赖，并返回一个上下文管理器<br>with g.control_dependencies([a, b, c]):<br># <code>d</code> 和 <code>e</code> 将在 <code>a</code>, <code>b</code>, 和<code>c</code>执行完之后运行.<br>d = …<br> e = …</td><br></tr><br><tr><br>  <td>tf.Graph.device(device_name_or_function)</td><br>  <td align="left">定义运行图所使用的设备，并返回一个上下文管理器<br><code>with g.device(‘/gpu:0’): …</code><br><code>with g.device(‘/cpu:0’): …</code></td><br></tr><br><tr><br>  <td>tf.Graph.name_scope(name)</td><br>  <td align="left">为节点创建层次化的名称，并返回一个上下文管理器</td><br></tr><br><tr><br>  <td>tf.Graph.add_to_collection(name, value)</td><br>  <td align="left">将value以name的名称存储在收集器(collection)中</td><br></tr><br><tr><br>  <td>tf.Graph.get_collection(name, scope=None)</td><br>  <td align="left">根据name返回一个收集器中所收集的值的列表</td><br></tr><br><tr><br>  <td>tf.Graph.as_graph_element<br>(obj, allow_tensor=True, allow_operation=True)</td><br>  <td align="left">返回一个图中与obj相关联的对象，为一个操作节点或者tensor数据</td><br></tr><br><tr><br>  <td>tf.Graph.get_operation_by_name(name)</td><br>  <td align="left">根据名称返回操作节点</td><br></tr><br><tr><br>  <td>tf.Graph.get_tensor_by_name(name)</td><br>  <td align="left">根据名称返回tensor数据</td><br></tr><br><tr><br>  <td>tf.Graph.get_operations()</td><br>  <td align="left">返回图中的操作节点列表</td><br></tr><br><tr><br>  <td>tf.Graph.gradient_override_map(op_type_map)</td><br>  <td align="left">用于覆盖梯度函数的上下文管理器</td><br></tr><br></tbody></table><br><br><br><pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment">#class tf.Graph</span><br><span class="hljs-comment">#tensorflow运行时需要设置默认的图</span><br>g = tf.Graph()<br><span class="hljs-keyword">with</span> g.as_default():<br>  <span class="hljs-comment"># Define operations and tensors in <code>g</code>.</span><br>  c = tf.constant(<span class="hljs-number">30.0</span>)<br>  <span class="hljs-keyword">assert</span> c.graph <span class="hljs-keyword">is</span> g<br><br><span class="hljs-comment">##也可以使用tf.get_default_graph()获得默认图，也可在基础上加入节点或子图</span><br>c = tf.constant(<span class="hljs-number">4.0</span>)<br><span class="hljs-keyword">assert</span> c.graph <span class="hljs-keyword">is</span> tf.get_default_graph()</code></pre><br><br><br><br><pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment">#tf.Graph.as_default</span><br><span class="hljs-comment">#以下两段代码功能相同</span><br><span class="hljs-comment">#1、使用Graph.as_default():</span><br>g = tf.Graph()<br><span class="hljs-keyword">with</span> g.as_default():<br>  c = tf.constant(<span class="hljs-number">5.0</span>)<br>  <span class="hljs-keyword">assert</span> c.graph <span class="hljs-keyword">is</span> g<br><br><span class="hljs-comment">#2、构造和设置为默认</span><br><span class="hljs-keyword">with</span> tf.Graph().as_default() <span class="hljs-keyword">as</span> g:<br>  c = tf.constant(<span class="hljs-number">5.0</span>)<br>  <span class="hljs-keyword">assert</span> c.graph <span class="hljs-keyword">is</span> g</code></pre><br><br><br><br><pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment">#tf.Graph.control_dependencies(control_inputs)</span><br><span class="hljs-comment"># 错误代码</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">my_func</span><span class="hljs-params">(pred, tensor)</span>:</span><br>  t = tf.matmul(tensor, tensor)<br>  <span class="hljs-keyword">with</span> tf.control_dependencies([pred]):<br>    <span class="hljs-comment"># 乘法操作(op)没有创建在该上下文，所以没有被加入依赖控制</span><br>    <span class="hljs-keyword">return</span> t<br><br><span class="hljs-comment"># 正确代码</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">my_func</span><span class="hljs-params">(pred, tensor)</span>:</span><br>  <span class="hljs-keyword">with</span> tf.control_dependencies([pred]):<br>    <span class="hljs-comment"># 乘法操作(op)创建在该上下文，所以被加入依赖控制中</span><br>    <span class="hljs-comment">#执行完pred之后再执行matmul</span><br>    <span class="hljs-keyword">return</span> tf.matmul(tensor, tensor)<br></code></pre><br><br><br><br><pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment"># tf.Graph.name_scope(name)</span><br><span class="hljs-comment"># 一个图中包含有一个名称范围的堆栈，在使用name_scope(…)之后，将压(push)新名称进栈中，</span><br><span class="hljs-comment">#并在下文中使用该名称</span><br><span class="hljs-keyword">with</span> tf.Graph().as_default() <span class="hljs-keyword">as</span> g:<br>  c = tf.constant(<span class="hljs-number">5.0</span>, name=<span class="hljs-string">“c”</span>)<br>  <span class="hljs-keyword">assert</span> c.op.name == <span class="hljs-string">“c”</span><br>  c_1 = tf.constant(<span class="hljs-number">6.0</span>, name=<span class="hljs-string">“c”</span>)<br>  <span class="hljs-keyword">assert</span> c_1.op.name == <span class="hljs-string">“c_1”</span><br><br>  <span class="hljs-comment"># Creates a scope called “nested”</span><br>  <span class="hljs-keyword">with</span> g.name_scope(<span class="hljs-string">“nested”</span>) <span class="hljs-keyword">as</span> scope:<br>    nested_c = tf.constant(<span class="hljs-number">10.0</span>, name=<span class="hljs-string">“c”</span>)<br>    <span class="hljs-keyword">assert</span> nested_c.op.name == <span class="hljs-string">“nested/c”</span><br><br>    <span class="hljs-comment"># Creates a nested scope called “inner”.</span><br>    <span class="hljs-keyword">with</span> g.name_scope(<span class="hljs-string">“inner”</span>):<br>      nested_inner_c = tf.constant(<span class="hljs-number">20.0</span>, name=<span class="hljs-string">“c”</span>)<br>      <span class="hljs-keyword">assert</span> nested_inner_c.op.name == <span class="hljs-string">“nested/inner/c”</span><br><br>    <span class="hljs-comment"># Create a nested scope called “inner_1”.</span><br>    <span class="hljs-keyword">with</span> g.name_scope(<span class="hljs-string">“inner”</span>):<br>      nested_inner_1_c = tf.constant(<span class="hljs-number">30.0</span>, name=<span class="hljs-string">“c”</span>)<br>      <span class="hljs-keyword">assert</span> nested_inner_1_c.op.name == <span class="hljs-string">“nested/inner_1/c”</span><br><br>      <span class="hljs-comment"># Treats <code>scope</code> as an absolute name scope, and</span><br>      <span class="hljs-comment"># switches to the “nested/“ scope.</span><br>      <span class="hljs-keyword">with</span> g.name_scope(scope):<br>        nested_d = tf.constant(<span class="hljs-number">40.0</span>, name=<span class="hljs-string">“d”</span>)<br>        <span class="hljs-keyword">assert</span> nested_d.op.name == <span class="hljs-string">“nested/d”</span><br><br>        <span class="hljs-keyword">with</span> g.name_scope(<span class="hljs-string">“”</span>):<br>          e = tf.constant(<span class="hljs-number">50.0</span>, name=<span class="hljs-string">“e”</span>)<br>          <span class="hljs-keyword">assert</span> e.op.name == <span class="hljs-string">“e”</span><br></code></pre><br><br><hr><br><br><p><strong><em>tf.Operation</em></strong></p><br><br><table><br><thead><br><tr><br>  <th>操作</th><br>  <th align="left">描述</th><br></tr><br></thead><br><tbody><tr><br>  <td>class tf.Operation</td><br>  <td align="left">代表图中的一个节点，用于计算tensors数据<br>该类型将由python节点构造器产生(比如tf.matmul())<br>或者Graph.create_op()<br>例如c = tf.matmul(a, b)创建一个Operation类<br>为类型为”MatMul”,输入为’a’,’b’，输出为’c’的操作类</td><br></tr><br><tr><br>  <td>tf.Operation.name</td><br>  <td align="left">操作节点(op)的名称</td><br></tr><br><tr><br>  <td>tf.Operation.type</td><br>  <td align="left">操作节点(op)的类型，比如”MatMul”</td><br></tr><br><tr><br>  <td>tf.Operation.inputs<br>tf.Operation.outputs</td><br>  <td align="left">操作节点的输入与输出</td><br></tr><br><tr><br>  <td>tf.Operation.control_inputs</td><br>  <td align="left">操作节点的依赖</td><br></tr><br><tr><br>  <td>tf.Operation.run(feed_dict=None, session=None)</td><br>  <td align="left">在会话(Session)中运行该操作</td><br></tr><br><tr><br>  <td>tf.Operation.get_attr(name)</td><br>  <td align="left">获取op的属性值</td><br></tr><br></tbody></table><br><br><br><hr><br><br><p><strong><em>tf.Tensor</em></strong></p><br><br><table><br><thead><br><tr><br>  <th>操作</th><br>  <th align="left">描述</th><br></tr><br></thead><br><tbody><tr><br>  <td>class tf.Tensor</td><br>  <td align="left">表示一个由操作节点op产生的值，<br>TensorFlow程序使用tensor数据结构来代表所有的数据, <br>计算图中, 操作间传递的数据都是 tensor，一个tensor是一个符号handle,<br>里面并没有表示实际数据，而相当于数据流的载体</td><br></tr><br><tr><br>  <td>tf.Tensor.dtype</td><br>  <td align="left">tensor中数据类型</td><br></tr><br><tr><br>  <td>tf.Tensor.name</td><br>  <td align="left">该tensor名称</td><br></tr><br><tr><br>  <td>tf.Tensor.value_index</td><br>  <td align="left">该tensor输出外op的index</td><br></tr><br><tr><br>  <td>tf.Tensor.graph</td><br>  <td align="left">该tensor所处在的图</td><br></tr><br><tr><br>  <td>tf.Tensor.op</td><br>  <td align="left">产生该tensor的op</td><br></tr><br><tr><br>  <td>tf.Tensor.consumers()</td><br>  <td align="left">返回使用该tensor的op列表</td><br></tr><br><tr><br>  <td>tf.Tensor.eval(feed_dict=None, session=None)</td><br>  <td align="left">在会话中求tensor的值<br>需要使用<code>with sess.as_default()</code>或者 <code>eval(session=sess)</code></td><br></tr><br><tr><br>  <td>tf.Tensor.get_shape()</td><br>  <td align="left">返回用于表示tensor的shape的类TensorShape</td><br></tr><br><tr><br>  <td>tf.Tensor.set_shape(shape)</td><br>  <td align="left">更新tensor的shape</td><br></tr><br><tr><br>  <td>tf.Tensor.device</td><br>  <td align="left">设置计算该tensor的设备</td><br></tr><br></tbody></table><br><br><br><br><br><pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment">#tf.Tensor.get_shape()</span><br>c = tf.constant([[<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>], [<span class="hljs-number">4.0</span>, <span class="hljs-number">5.0</span>, <span class="hljs-number">6.0</span>]])<br>print(c.get_shape())<br>==&gt; TensorShape([Dimension(<span class="hljs-number">2</span>), Dimension(<span class="hljs-number">3</span>)])</code></pre><br><br><br><br><pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment">#现在有个用于图像处理的tensor-&gt;image</span><br>print(image.get_shape())<br>==&gt; TensorShape([Dimension(<span class="hljs-keyword">None</span>), Dimension(<span class="hljs-keyword">None</span>), Dimension(<span class="hljs-number">3</span>)])<br><span class="hljs-comment"># 假如我们知道数据集中图像尺寸为28 x 28，那么可以设置</span><br>image.set_shape([<span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">3</span>])<br>print(image.get_shape())<br>==&gt; TensorShape([Dimension(<span class="hljs-number">28</span>), Dimension(<span class="hljs-number">28</span>), Dimension(<span class="hljs-number">3</span>)])</code></pre><br><br><hr><br><br><br><br><h4 id="tensor类型tensor-types"> <strong>tensor类型(Tensor types)</strong></h4><br><br><p><strong><em>tf.DType</em></strong></p><br><br><table><br><thead><br><tr><br>  <th>操作</th><br>  <th align="left">描述</th><br></tr><br></thead><br><tbody><tr><br>  <td>class tf.DType</td><br>  <td align="left">数据类型主要包含<br>tf.float16，tf.float16,tf.float32,tf.float64,<br>tf.bfloat16,tf.complex64,tf.complex128,<br>tf.int8,tf.uint8,tf.uint16,tf.int16,tf.int32,<br>tf.int64,tf.bool,tf.string</td><br></tr><br><tr><br>  <td>tf.DType.is_compatible_with(other)</td><br>  <td align="left">判断other的数据类型是否将转变为该DType</td><br></tr><br><tr><br>  <td>tf.DType.name</td><br>  <td align="left">数据类型名称</td><br></tr><br><tr><br>  <td>tf.DType.base_dtype</td><br>  <td align="left">返回该DType的基础DType，而非参考的数据类型(non-reference)</td><br></tr><br><tr><br>  <td>tf.DType.as_ref</td><br>  <td align="left">返回一个基于DType的参考数据类型</td><br></tr><br><tr><br>  <td>tf.DType.is_floating</td><br>  <td align="left">判断是否为浮点类型</td><br></tr><br><tr><br>  <td>tf.DType.is_complex</td><br>  <td align="left">判断是否为复数</td><br></tr><br><tr><br>  <td>tf.DType.is_integer</td><br>  <td align="left">判断是否为整数</td><br></tr><br><tr><br>  <td>tf.DType.is_unsigned</td><br>  <td align="left">判断是否为无符号型数据</td><br></tr><br><tr><br>  <td>tf.DType.as_numpy_dtype</td><br>  <td align="left">返回一个基于DType的numpy.dtype类型</td><br></tr><br><tr><br>  <td>tf.DType.max<br>tf.DType.min</td><br>  <td align="left">返回这种数据类型能表示的最大值及其最小值</td><br></tr><br><tr><br>  <td>tf.as_dtype(type_value)</td><br>  <td align="left">返回由type_value转变得的相应tf数据类型</td><br></tr><br></tbody></table><br><br><br><hr><br><br><hr><br><br><h4 id="通用函数utility-functions"><em> <strong>通用函数（Utility functions）</strong></em></h4><br><br><table><br><thead><br><tr><br>  <th>操作</th><br>  <th align="left">描述</th><br></tr><br></thead><br><tbody><tr><br>  <td>tf.device(device_name_or_function)</td><br>  <td align="left">基于默认的图，其功能便为Graph.device()</td><br></tr><br><tr><br>  <td>tf.container(container_name)</td><br>  <td align="left">基于默认的图，其功能便为Graph.container()</td><br></tr><br><tr><br>  <td>tf.name_scope(name)</td><br>  <td align="left">基于默认的图，其功能便为 Graph.name_scope()</td><br></tr><br><tr><br>  <td>tf.control_dependencies(control_inputs)</td><br>  <td align="left">基于默认的图，其功能便为Graph.control_dependencies()</td><br></tr><br><tr><br>  <td><strong>tf.convert_to_tensor<br>(value, dtype=None, name=None, as_ref=False)</strong></td><br>  <td align="left">将value转变为tensor数据类型</td><br></tr><br><tr><br>  <td>tf.get_default_graph()</td><br>  <td align="left">返回返回当前线程的默认图</td><br></tr><br><tr><br>  <td>tf.reset_default_graph()</td><br>  <td align="left">清除默认图的堆栈，并设置全局图为默认图</td><br></tr><br><tr><br>  <td>tf.import_graph_def(graph_def, input_map=None,<br> return_elements=None, name=None, op_dict=None,<br> producer_op_list=None)</td><br>  <td align="left">将graph_def的图导入到python中</td><br></tr><br></tbody></table><br><br><br><hr><br><br><h4 id="图收集graph-collections"> <strong>图收集（Graph collections）</strong></h4><br><br><table><br><thead><br><tr><br>  <th>操作</th><br>  <th align="left">描述</th><br></tr><br></thead><br><tbody><tr><br>  <td>tf.add_to_collection(name, value)</td><br>  <td align="left">基于默认的图，其功能便为Graph.add_to_collection()</td><br></tr><br><tr><br>  <td>tf.get_collection(key, scope=None)</td><br>  <td align="left">基于默认的图，其功能便为Graph.get_collection()</td><br></tr><br></tbody></table><br><br><br><hr><br><br><h4 id="定义新操作节点defining-new-operations"><em> <strong>定义新操作节点（Defining new operations）</strong></em></h4><br><br><p><strong><em>tf.RegisterGradient</em></strong></p><br><br><table><br><thead><br><tr><br>  <th>操作</th><br>  <th align="left">描述</th><br></tr><br></thead><br><tbody><tr><br>  <td>class tf.RegisterGradient</td><br>  <td align="left">返回一个用于寄存op类型的梯度函数的装饰器</td><br></tr><br><tr><br>  <td>tf.NoGradient(op_type)</td><br>  <td align="left">设置操作节点类型op_type的节点没有指定的梯度</td><br></tr><br><tr><br>  <td>class tf.RegisterShape</td><br>  <td align="left">返回一个用于寄存op类型的shape函数的装饰器</td><br></tr><br><tr><br>  <td>class tf.TensorShape</td><br>  <td align="left">表示tensor的shape</td><br></tr><br><tr><br>  <td>tf.TensorShape.merge_with(other)</td><br>  <td align="left">与other合并shape信息，返回一个TensorShape类</td><br></tr><br><tr><br>  <td>tf.TensorShape.concatenate(other)</td><br>  <td align="left">与other的维度相连结</td><br></tr><br><tr><br>  <td>tf.TensorShape.ndims</td><br>  <td align="left">返回tensor的rank</td><br></tr><br><tr><br>  <td>tf.TensorShape.dims</td><br>  <td align="left">返回tensor的维度</td><br></tr><br><tr><br>  <td>tf.TensorShape.as_list()</td><br>  <td align="left">以list的形式返回tensor的shape</td><br></tr><br><tr><br>  <td>tf.TensorShape.is_compatible_with(other)</td><br>  <td align="left">判断shape是否为兼容<br>TensorShape(None)与其他任何shape值兼容</td><br></tr><br><tr><br>  <td>class tf.Dimension</td><br>  <td align="left"></td><br></tr><br><tr><br>  <td>tf.Dimension.is_compatible_with(other)</td><br>  <td align="left">判断dims是否为兼容</td><br></tr><br><tr><br>  <td>tf.Dimension.merge_with(other)</td><br>  <td align="left">与other合并dims信息</td><br></tr><br><tr><br>  <td><strong>tf.op_scope(values, name, default_name=None)</strong></td><br>  <td align="left">在python定义op时，返回一个上下文管理器</td><br></tr><br></tbody></table><br><br><br><pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment">#tf.RegisterGradient</span><br><span class="hljs-comment">#该装饰器只使用于定义一个新的op类型时候，如果一个op有m个输入，n个输出。那么该梯度函数应该设置原始的</span><br><span class="hljs-comment">#操作类型，以及n个Tensor对象（表示每一个op输出的梯度），以及m个对象(表示每一个op输入的偏梯度)</span><br><span class="hljs-comment">#以操作节点类型为’Sub’为例，两输入为x,y。为一个输出x-y</span><br><span class="hljs-decorator">@tf.RegisterGradient(“Sub”)</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_sub_grad</span><span class="hljs-params">(unused_op, grad)</span>:</span><br>  <span class="hljs-keyword">return</span> grad, tf.neg(grad)</code></pre><br><br><br><br><pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment">#tf.op_scope</span><br><span class="hljs-comment">#定义一个名称为my_op的python操作节点op</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">my_op</span><span class="hljs-params">(a, b, c, name=None)</span>:</span><br>  <span class="hljs-keyword">with</span> tf.op_scope([a, b, c], name, <span class="hljs-string">“MyOp”</span>) <span class="hljs-keyword">as</span> scope:<br>    a = tf.convert_to_tensor(a, name=<span class="hljs-string">“a”</span>)<br>    b = tf.convert_to_tensor(b, name=<span class="hljs-string">“b”</span>)<br>    c = tf.convert_to_tensor(c, name=<span class="hljs-string">“c”</span>)<br>    <span class="hljs-comment"># Define some computation that uses <code>a</code>, <code>b</code>, and <code>c</code>.</span><br>    <span class="hljs-keyword">return</span> foo_op(…, name=scope)</code></pre><br><br><hr><br><br><hr><br><br><br><br><h3 id="22-输入和读取器inputs-and-readers">2.2 <strong>输入和读取器(Inputs and Readers)</strong></h3><br><br><p>本节主要介绍tensorflow中数据的读入相关类或函数</p><br><br><hr><br><br><h4 id="占位符placeholders"> <strong>占位符（Placeholders）</strong></h4><br><br><p>tf提供一种占位符操作，在执行时需要为其提供数据data。</p><br><br><table><br><thead><br><tr><br>  <th>操作</th><br>  <th align="left">描述</th><br></tr><br></thead><br><tbody><tr><br>  <td>tf.placeholder(dtype, shape=None, name=None)</td><br>  <td align="left">为一个tensor插入一个占位符<br>eg:x = tf.placeholder(tf.float32, shape=(1024, 1024))</td><br></tr><br><tr><br>  <td>tf.placeholder_with_default(input, shape, name=None)</td><br>  <td align="left">当输出没有fed时，input通过一个占位符op</td><br></tr><br><tr><br>  <td>tf.sparse_placeholder(dtype, shape=None, name=None)</td><br>  <td align="left">为一个稀疏tensor插入一个占位符</td><br></tr><br></tbody></table><br><br><br><hr><br><br><h4 id="读取器readers"><em> <strong>读取器（Readers）</strong></em></h4><br><br><p>tf提供一系列读取各种数据格式的类。对于多文件输入，可以使用函数<a href="https://www.tensorflow.org/versions/r0.10/api_docs/python/io_ops.html#string_input_producer" target="_blank" rel="noopener">tf.train.string_input_producer</a>，该函数将创建一个保持文件的FIFO队列，以供reader使用。或者如果输入的这些文件名有相雷同的字符串，也可以使用函数<a href="https://www.tensorflow.org/versions/r0.10/api_docs/python/io_ops.html#match_filenames_once" target="_blank" rel="noopener">tf.train.match_filenames_once</a>。</p><br><br><table><br><thead><br><tr><br>  <th>操作</th><br>  <th align="left">描述</th><br></tr><br></thead><br><tbody><tr><br>  <td>class tf.ReaderBase</td><br>  <td align="left">不同的读取器类型的基本类</td><br></tr><br><tr><br>  <td>tf.ReaderBase.read(queue, name=None)</td><br>  <td align="left">返回下一个记录对(key, value),queue为tf文件队列FIFOQueue</td><br></tr><br><tr><br>  <td>tf.ReaderBase.read_up_to(queue, num_records, name=None)</td><br>  <td align="left">返回reader产生的num_records对(key, value)</td><br></tr><br><tr><br>  <td>tf.ReaderBase.reader_ref</td><br>  <td align="left">返回应用在该reader上的Op</td><br></tr><br><tr><br>  <td>tf.ReaderBase.reset(name=None)</td><br>  <td align="left">恢复reader为初始状态</td><br></tr><br><tr><br>  <td>tf.ReaderBase.restore_state(state, name=None)</td><br>  <td align="left">恢复reader为之前的保存状态state</td><br></tr><br><tr><br>  <td>tf.ReaderBase.serialize_state(name=None)</td><br>  <td align="left">返回一个reader解码后产生的字符串tansor</td><br></tr><br><tr><br>  <td>class tf.TextLineReader</td><br>  <td align="left"></td><br></tr><br><tr><br>  <td>tf.TextLineReader.num_records_produced(name=None)</td><br>  <td align="left">返回reader已经产生的记录(records )数目</td><br></tr><br><tr><br>  <td>tf.TextLineReader.num_work_units_completed(name=None)</td><br>  <td align="left">返回该reader已经完成的处理的work数目</td><br></tr><br><tr><br>  <td>tf.TextLineReader.read(queue, name=None)</td><br>  <td align="left">返回reader所产生的下一个记录对 (key, value)，该reader可以限定新产生输出的行数</td><br></tr><br><tr><br>  <td>tf.TextLineReader.reader_ref</td><br>  <td align="left">返回应用在该reader上的Op</td><br></tr><br><tr><br>  <td>tf.TextLineReader.reset(name=None)</td><br>  <td align="left">恢复reader为初始状态</td><br></tr><br><tr><br>  <td>tf.TextLineReader.restore_state(state, name=None)</td><br>  <td align="left">恢复reader为之前的保存状态state</td><br></tr><br><tr><br>  <td>tf.TextLineReader.serialize_state(name=None)</td><br>  <td align="left">返回一个reader解码后产生的字符串tansor</td><br></tr><br><tr><br>  <td>class tf.WholeFileReader</td><br>  <td align="left">一个阅读器，读取整个文件，返回文件名称key,以及文件中所有的内容value,该类的方法同上，不赘述</td><br></tr><br><tr><br>  <td>class tf.IdentityReader</td><br>  <td align="left">一个reader，以key和value的形式，输出一个work队列。该类其他方法基本同上</td><br></tr><br><tr><br>  <td>class tf.TFRecordReader</td><br>  <td align="left">读取TFRecord格式文件的reader。该类其他方法基本同上</td><br></tr><br><tr><br>  <td>class tf.FixedLengthRecordReader</td><br>  <td align="left">输出</td><br></tr><br></tbody></table><br><br><br><hr><br><br><h4 id="数据转换converting"> <strong>数据转换（Converting）</strong></h4><br><br><p>tf提供一系列方法将各种格式数据转换为tensor表示。</p><br><br><table><br><thead><br><tr><br>  <th>操作</th><br>  <th align="left">描述</th><br></tr><br></thead><br><tbody><tr><br>  <td>tf.decode_csv(records, record_defaults, <br>field_delim=None, name=None)</td><br>  <td align="left">将csv转换为tensor，与tf.TextLineReader搭配使用</td><br></tr><br><tr><br>  <td>tf.decode_raw(bytes, out_type, <br>little_endian=None, name=None)</td><br>  <td align="left">将bytes转换为一个数字向量表示，bytes为一个字符串类型的tensor<br>与函数 tf.FixedLengthRecordReader搭配使用，详见<a href="https://www.tensorflow.org/code/tensorflow/models/image/cifar10/cifar10_input.py" target="_blank" rel="noopener">tf的CIFAR-10例子</a></td><br></tr><br></tbody></table><br><br><br><p>选取与要输入的文件格式相匹配的reader，并将文件队列提供给reader的读方法( read method)。读方法将返回文件唯一标识的key，以及一个记录(record)（有助于对出现一些另类的records时debug），以及一个标量的字符串值。再使用一个（或多个）解码器(decoder) 或转换操作(conversion ops)将字符串转换为tensor类型。</p><br><br><pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment">#读取文件队列，使用reader中read的方法，返回key与value</span><br>filename_queue = tf.train.string_input_producer([<span class="hljs-string">“file0.csv”</span>, <span class="hljs-string">“file1.csv”</span>])<br>reader = tf.TextLineReader()<br>key, value = reader.read(filename_queue)<br><br>record_defaults = [[<span class="hljs-number">1</span>], [<span class="hljs-number">1</span>], [<span class="hljs-number">1</span>], [<span class="hljs-number">1</span>], [<span class="hljs-number">1</span>]]<br>col1, col2, col3, col4, col5 = tf.decode_csv(<br>    value, record_defaults=record_defaults)<br>features = tf.pack([col1, col2, col3, col4])<br><br><span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:<br>  <span class="hljs-comment"># Start populating the filename queue.</span><br>  coord = tf.train.Coordinator()<br>  threads = tf.train.start_queue_runners(coord=coord)<br><br>  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1200</span>):<br>    <span class="hljs-comment"># Retrieve a single instance:</span><br>    example, label = sess.run([features, col5])<br><br>  coord.request_stop()<br>  coord.join(threads)</code></pre><br><br><hr><br><br><br><br><h4 id="example-protocol-buffer"><em> <strong>Example protocol buffer</strong></em></h4><br><br><p>提供了一些<a href="https://www.tensorflow.org/code/tensorflow/core/example/example.proto" target="_blank" rel="noopener">Example protocol buffers</a>，tf所<a href="https://www.tensorflow.org/versions/r0.10/how_tos/reading_data/index.html#standard-tensorflow-format" target="_blank" rel="noopener">推荐的用于训练样本的数据格式</a>，它们包含特征信息，<a href="https://www.tensorflow.org/code/tensorflow/core/example/feature.proto" target="_blank" rel="noopener">详情可见</a>。 <br><br>这是一种与前述将手上现有的各种数据类型转换为支持的格式的方法，这种方法更容易将网络结构与数据集融合或匹配。这种tensorflow所推荐的数据格式是一个包含tf.train.Example protocol buffers (包含特征<a href="https://www.tensorflow.org/code/tensorflow/core/example/feature.proto" target="_blank" rel="noopener">Features</a>域)的TFRecords文件。 <br><br>1、获取这种格式的文件方式为，首先将一般的数据格式填入Example protocol buffer中，再将 protocol buffer序列化为一个字符串，然后使用tf.python_io.TFRecordWriter类的相关方法将字符串写入一个TFRecords文件中，<a href="https://www.tensorflow.org/code/tensorflow/examples/how_tos/reading_data/convert_to_records.py" target="_blank" rel="noopener">参见MNIST例子</a>，将MNIST 数据转换为该类型数据。 <br><br>2、读取TFRecords格式文件的方法为，使用tf.TFRecordReader读取器和tf.parse_single_example解码器。parse_single_example操作将 example protocol buffers解码为tensor形式。<a href="https://www.tensorflow.org/code/tensorflow/examples/how_tos/reading_data/fully_connected_reader.py" target="_blank" rel="noopener">参见MNIST例子</a></p><br><br><table><br><thead><br><tr><br>  <th>操作</th><br>  <th align="left">描述</th><br></tr><br></thead><br><tbody><tr><br>  <td>class tf.VarLenFeature</td><br>  <td align="left">解析变长的输入特征feature相关配置</td><br></tr><br><tr><br>  <td>class tf.FixedLenFeature</td><br>  <td align="left">解析定长的输入特征feature相关配置</td><br></tr><br><tr><br>  <td>class tf.FixedLenSequenceFeature</td><br>  <td align="left">序列项目中的稠密(dense )输入特征的相关配置</td><br></tr><br><tr><br>  <td><strong>tf.parse_example(serialized, features, <br>name=None, example_names=None)</strong></td><br>  <td align="left">将一组Example protos解析为tensor的字典形式<br>解析serialized所给予的序列化的一些Example protos<br>返回一个由特征keys映射Tensor和SparseTensor值的字典</td><br></tr><br><tr><br>  <td><strong>tf.parse_single_example(serialized, features, <br>name=None, example_names=None)</strong></td><br>  <td align="left">解析一个单独的Example proto，与tf.parse_example方法雷同</td><br></tr><br><tr><br>  <td>tf.decode_json_example(json_examples, name=None)</td><br>  <td align="left">将JSON编码的样本记录转换为二进制protocol buffer字符串</td><br></tr><br></tbody></table><br><br><br><pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment">#tf.parse_example的使用举例</span><br><span class="hljs-comment">#输入序列化数据如下： </span><br>serialized = [<br>  features<br>    { feature { key: <span class="hljs-string">“ft”</span> value { float_list { value: [<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>] } } } },<br>  features<br>    { feature []},<br>  features<br>    { feature { key: <span class="hljs-string">“ft”</span> value { float_list { value: [<span class="hljs-number">3.0</span>] } } }<br>]<br><span class="hljs-comment">#那么输出为一个字典(dict),如下：</span><br>{<span class="hljs-string">“ft”</span>: SparseTensor(indices=[[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">0</span>]],<br>                    values=[<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>],<br>                    shape=(<span class="hljs-number">3</span>, <span class="hljs-number">2</span>)) }<br><span class="hljs-comment">#########</span><br><span class="hljs-comment">#再来看一个例子，给定两个序列化的原始输入样本：</span><br>[<br>  features {<br>    feature { key: <span class="hljs-string">“kw”</span> value { bytes_list { value: [ <span class="hljs-string">“knit”</span>, <span class="hljs-string">“big”</span> ] } } }<br>    feature { key: <span class="hljs-string">“gps”</span> value { float_list { value: [] } } }<br>  },<br>  features {<br>    feature { key: <span class="hljs-string">“kw”</span> value { bytes_list { value: [ <span class="hljs-string">“emmy”</span> ] } } }<br>    feature { key: <span class="hljs-string">“dank”</span> value { int64_list { value: [ <span class="hljs-number">42</span> ] } } }<br>    feature { key: <span class="hljs-string">“gps”</span> value { } }<br>  }<br>]<br><span class="hljs-comment">#相关参数如下：</span><br>example_names: [<span class="hljs-string">“input0”</span>, <span class="hljs-string">“input1”</span>],<br>features: {<br>    <span class="hljs-string">“kw”</span>: VarLenFeature(tf.string),<br>    <span class="hljs-string">“dank”</span>: VarLenFeature(tf.int64),<br>    <span class="hljs-string">“gps”</span>: VarLenFeature(tf.float32),<br>}<br><span class="hljs-comment">#那么有如下输出：</span><br>{<br>  <span class="hljs-string">“kw”</span>: SparseTensor(<br>      indices=[[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>]],<br>      values=[<span class="hljs-string">“knit”</span>, <span class="hljs-string">“big”</span>, <span class="hljs-string">“emmy”</span>]<br>      shape=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>]),<br>  <span class="hljs-string">“dank”</span>: SparseTensor(<br>      indices=[[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>]],<br>      values=[<span class="hljs-number">42</span>],<br>      shape=[<span class="hljs-number">2</span>, <span class="hljs-number">1</span>]),<br>  <span class="hljs-string">“gps”</span>: SparseTensor(<br>      indices=[],<br>      values=[],<br>      shape=[<span class="hljs-number">2</span>, <span class="hljs-number">0</span>]),<br>}<br><span class="hljs-comment">#########</span><br><span class="hljs-comment">#对于两个样本的输出稠密结果情况</span><br>[<br>  features {<br>    feature { key: <span class="hljs-string">“age”</span> value { int64_list { value: [ <span class="hljs-number">0</span> ] } } }<br>    feature { key: <span class="hljs-string">“gender”</span> value { bytes_list { value: [ <span class="hljs-string">“f”</span> ] } } }<br>   },<br>   features {<br>    feature { key: <span class="hljs-string">“age”</span> value { int64_list { value: [] } } }<br>    feature { key: <span class="hljs-string">“gender”</span> value { bytes_list { value: [ <span class="hljs-string">“f”</span> ] } } }<br>  }<br>]<br><span class="hljs-comment">#我们可以使用以下参数</span><br>example_names: [<span class="hljs-string">“input0”</span>, <span class="hljs-string">“input1”</span>],<br>features: {<br>    <span class="hljs-string">“age”</span>: FixedLenFeature([], dtype=tf.int64, default_value=-<span class="hljs-number">1</span>),<br>    <span class="hljs-string">“gender”</span>: FixedLenFeature([], dtype=tf.string),<br>}<br><span class="hljs-comment">#期望的结果如下</span><br>{<br>  <span class="hljs-string">“age”</span>: [[<span class="hljs-number">0</span>], [-<span class="hljs-number">1</span>]],<br>  <span class="hljs-string">“gender”</span>: [[<span class="hljs-string">“f”</span>], [<span class="hljs-string">“f”</span>]],<br>}</code></pre><br><br><br><br><pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment">##Example protocol buffer相关使用的例子</span><br><span class="hljs-comment">#将mnist的数据转换为TFRecords文件格式</span><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><span class="hljs-keyword">from</span> tensorflow.contrib.learn.python.learn.datasets <span class="hljs-keyword">import</span> mnist<br>SOURCE_URL = <span class="hljs-string">‘<a href="http://yann.lecun.com/exdb/mnist/&#39;" target="_blank" rel="noopener">http://yann.lecun.com/exdb/mnist/&#39;</a></span><br><br>TRAIN_IMAGES = <span class="hljs-string">‘train-images-idx3-ubyte.gz’</span>  <span class="hljs-comment"># MNIST filenames</span><br>TRAIN_LABELS = <span class="hljs-string">‘train-labels-idx1-ubyte.gz’</span><br>TEST_IMAGES = <span class="hljs-string">‘t10k-images-idx3-ubyte.gz’</span><br>TEST_LABELS = <span class="hljs-string">‘t10k-labels-idx1-ubyte.gz’</span><br><br>tf.app.flags.DEFINE_string(<span class="hljs-string">‘directory’</span>, <span class="hljs-string">‘/tmp/data’</span>,<br>                           <span class="hljs-string">‘Directory to download data files and write the ‘</span><br>                           <span class="hljs-string">‘converted result’</span>)<br>tf.app.flags.DEFINE_integer(<span class="hljs-string">‘validation_size’</span>, <span class="hljs-number">5000</span>,<br>                            <span class="hljs-string">‘Number of examples to separate from the training ‘</span><br>                            <span class="hljs-string">‘data for the validation set.’</span>)<br>FLAGS = tf.app.flags.FLAGS<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_int64_feature</span><span class="hljs-params">(value)</span>:</span><br>  <span class="hljs-keyword">return</span> tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_bytes_feature</span><span class="hljs-params">(value)</span>:</span><br>  <span class="hljs-keyword">return</span> tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">convert_to</span><span class="hljs-params">(data_set, name)</span>:</span><br>  images = data_set.images<br>  labels = data_set.labels<br>  num_examples = data_set.num_examples<br><br>  <span class="hljs-keyword">if</span> images.shape[<span class="hljs-number">0</span>] != num_examples:<br>    <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">‘Images size %d does not match label size %d.’</span> %<br>                     (images.shape[<span class="hljs-number">0</span>], num_examples))<br>  rows = images.shape[<span class="hljs-number">1</span>]<br>  cols = images.shape[<span class="hljs-number">2</span>]<br>  depth = images.shape[<span class="hljs-number">3</span>]<br><br>  filename = os.path.join(FLAGS.directory, name + <span class="hljs-string">‘.tfrecords’</span>)<br>  print(<span class="hljs-string">‘Writing’</span>, filename)<br>  writer = tf.python_io.TFRecordWriter(filename)<br>  <span class="hljs-keyword">for</span> index <span class="hljs-keyword">in</span> range(num_examples):<br>    image_raw = images[index].tostring()<br>    example = tf.train.Example(features=tf.train.Features(feature={<br>        <span class="hljs-string">‘height’</span>: _int64_feature(rows),<br>        <span class="hljs-string">‘width’</span>: _int64_feature(cols),<br>        <span class="hljs-string">‘depth’</span>: _int64_feature(depth),<br>        <span class="hljs-string">‘label’</span>: _int64_feature(int(labels[index])),<br>        <span class="hljs-string">‘image_raw’</span>: _bytes_feature(image_raw)}))<br>    writer.write(example.SerializeToString())<br>  writer.close()<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">main</span><span class="hljs-params">(argv)</span>:</span><br>  <span class="hljs-comment"># Get the data.</span><br>  data_sets = mnist.read_data_sets(FLAGS.directory,<br>                                   dtype=tf.uint8,<br>                                   reshape=<span class="hljs-keyword">False</span>)<br><br>  <span class="hljs-comment"># Convert to Examples and write the result to TFRecords.</span><br>  convert_to(data_sets.train, <span class="hljs-string">‘train’</span>)<br>  convert_to(data_sets.validation, <span class="hljs-string">‘validation’</span>)<br>  convert_to(data_sets.test, <span class="hljs-string">‘test’</span>)<br><br><span class="hljs-keyword">if</span> <strong>name</strong> == <span class="hljs-string">‘<strong>main</strong>‘</span>:<br>  tf.app.run()<br></code></pre><br><br><hr><br><br><br><br><h4 id="队列queues"> <strong>队列(Queues)</strong></h4><br><br><p>tensorflow提供了几个队列应用，用来将tf计算图与tensors的阶段流水组织到一起。队列是使用tensorflow计算的一个强大的机制，正如其他Tensorflow的元素一样，一个队列也是tf图中的一个节点(node),它是一个有状态的node，就像一个变量：其他节点可以改变其内容。 <br><br>我们来看一个简单的例子，如下gif图，我们将创建一个先入先出队列(FIFOQueue)并且将值全设为0，然后我们构建一个图以获取队列出来的元素，对该元素加1操作，并将结果再放入队列末尾。渐渐地，队列中的数字便增加。 <br><br><img data-src="http://img.blog.csdn.net/20160815145525960" alt="这里写图片描述" title></p><br><br><table><br><thead><br><tr><br>  <th>操作</th><br>  <th align="left">描述</th><br></tr><br></thead><br><tbody><tr><br>  <td>class tf.QueueBase</td><br>  <td align="left">基本的队列应用类.队列(queue)是一种数据结构，<br>该结构通过多个步骤存储tensors,<br>并且对tensors进行入列(enqueue)与出列(dequeue)操作</td><br></tr><br><tr><br>  <td>tf.QueueBase.enqueue(vals, name=None)</td><br>  <td align="left">将一个元素编入该队列中。如果在执行该操作时队列已满，<br>那么将会阻塞直到元素编入队列之中</td><br></tr><br><tr><br>  <td>tf.QueueBase.enqueue_many(vals, name=None)</td><br>  <td align="left">将零个或多个元素编入该队列中</td><br></tr><br><tr><br>  <td>tf.QueueBase.dequeue(name=None)</td><br>  <td align="left">将元素从队列中移出。如果在执行该操作时队列已空，<br>那么将会阻塞直到元素出列，返回出列的tensors的tuple</td><br></tr><br><tr><br>  <td>tf.QueueBase.dequeue_many(n, name=None)</td><br>  <td align="left">将一个或多个元素从队列中移出</td><br></tr><br><tr><br>  <td>tf.QueueBase.size(name=None)</td><br>  <td align="left">计算队列中的元素个数</td><br></tr><br><tr><br>  <td>tf.QueueBase.close<br>(cancel_pending_enqueues=False, name=None)</td><br>  <td align="left">关闭该队列</td><br></tr><br><tr><br>  <td>f.QueueBase.dequeue_up_to(n, name=None)</td><br>  <td align="left">从该队列中移出n个元素并将之连接</td><br></tr><br><tr><br>  <td>tf.QueueBase.dtypes</td><br>  <td align="left">列出组成元素的数据类型</td><br></tr><br><tr><br>  <td>tf.QueueBase.from_list(index, queues)</td><br>  <td align="left">根据queues[index]的参考队列创建一个队列</td><br></tr><br><tr><br>  <td>tf.QueueBase.name</td><br>  <td align="left">返回最队列下面元素的名称</td><br></tr><br><tr><br>  <td>tf.QueueBase.names</td><br>  <td align="left">返回队列每一个组成部分的名称</td><br></tr><br><tr><br>  <td><strong>class tf.FIFOQueue</strong></td><br>  <td align="left">在出列时依照先入先出顺序，其他方法与tf.QueueBase雷同</td><br></tr><br><tr><br>  <td>class tf.PaddingFIFOQueue</td><br>  <td align="left">一个FIFOQueue ，同时根据padding支持batching变长的tensor</td><br></tr><br><tr><br>  <td><strong>class tf.RandomShuffleQueue</strong></td><br>  <td align="left">该队列将随机元素出列，其他方法与tf.QueueBase雷同</td><br></tr><br></tbody></table><br><br><br><hr><br><br><h4 id="文件系统的处理dealing-with-the-filesystem"><em> <strong>文件系统的处理(Dealing with the filesystem)</strong></em></h4><br><br><table><br><thead><br><tr><br>  <th>操作</th><br>  <th align="left">描述</th><br></tr><br></thead><br><tbody><tr><br>  <td>tf.matching_files(pattern, name=None)</td><br>  <td align="left">返回与pattern匹配模式的文件名称</td><br></tr><br><tr><br>  <td><strong>tf.read_file(filename, name=None)</strong></td><br>  <td align="left">读取并输出输入文件的整个内容</td><br></tr><br></tbody></table><br><br><br><hr><br><br><h4 id="输入管道input-pipeline"> <strong>输入管道(Input pipeline)</strong></h4><br><br><p>用于设置输入预取数的管道TF函数，函数 “producer”添加一个队列至图中，同时一个相应用于运行队列中子图(subgraph)的QueueRunner </p><br><br><table><br><thead><br><tr><br>  <th>操作</th><br>  <th align="left">描述</th><br></tr><br></thead><br><tbody><tr><br>  <td>tf.train.match_filenames_once(pattern, name=None)</td><br>  <td align="left">保存与pattern的文件列表</td><br></tr><br><tr><br>  <td>tf.train.limit_epochs(tensor, num_epochs=None, name=None)</td><br>  <td align="left">返回一个num_epochs次数，然后报告OutOfRange错误</td><br></tr><br><tr><br>  <td>tf.train.input_producer(input_tensor, element_shape=None, <br>num_epochs=None, shuffle=True, seed=None, capacity=32, <br>shared_name=None, summary_name=None, name=None)</td><br>  <td align="left">为一个输入管道输出input_tensor中的多行至一个队列中</td><br></tr><br><tr><br>  <td>tf.train.range_input_producer(limit, num_epochs=None, <br>shuffle=True, seed=None, capacity=32, <br>shared_name=None, name=None)</td><br>  <td align="left">产生一个从1至limit-1的整数至队列中</td><br></tr><br><tr><br>  <td>tf.train.slice_input_producer(tensor_list, num_epochs=None, <br>shuffle=True, seed=None, capacity=32, <br>shared_name=None, name=None)</td><br>  <td align="left">对tensor_list中的每一个tensor切片</td><br></tr><br><tr><br>  <td><strong>tf.train.string_input_producer(string_tensor, num_epochs=None,<br> shuffle=True, seed=None, capacity=32, <br>shared_name=None, name=None)</strong></td><br>  <td align="left">为一个输入管道输出一组字符串(比如文件名)至队列中</td><br></tr><br></tbody></table><br><br><br><hr><br><br><h4 id="在输入管道末端批量打包batching-at-the-end-of-an-input-pipeline"><em> <strong>在输入管道末端批量打包(Batching at the end of an input pipeline)</strong></em></h4><br><br><p>该相关函数增添一个队列至图中以将数据一样本打包为batch。它们也会添加 一个QueueRunner，以便执行的已经被填满队列的子图</p><br><br><table><br><thead><br><tr><br>  <th>操作</th><br>  <th align="left">描述</th><br></tr><br></thead><br><tbody><tr><br>  <td>tf.train.batch(tensors, batch_size, num_threads=1,<br> capacity=32, enqueue_many=False, shapes=None, <br>dynamic_pad=False, allow_smaller_final_batch=False, <br>shared_name=None, name=None)</td><br>  <td align="left">在输入的tensors中创建一些tensor数据格式的batch，<br>若输入为shape[, x, y, z]，那么输出则为[batch_size, x, y, z]<br>返回一个列表或者一个具有与输入tensors相同类型tensors的字典</td><br></tr><br><tr><br>  <td>tf.train.batch_join(tensors_list, batch_size, <br>capacity=32, enqueue_many=False, shapes=None, <br>dynamic_pad=False, allow_smaller_final_batch=False, <br>shared_name=None, name=None)</td><br>  <td align="left">将一个tensors的列表添加至一个队列中以创建样本的batches<br>len(tensors_list)个线程将启动，<br>线程i将tensors_list[i]的tensors入列<br>tensors_list[i1][j]与tensors_list[i2][j]有相同的类型和shape</td><br></tr><br><tr><br>  <td><strong>tf.train.shuffle_batch(tensors, batch_size, capacity, <br>min_after_dequeue, num_threads=1, seed=None, <br>enqueue_many=False, shapes=None, <br>allow_smaller_final_batch=False,<br> shared_name=None, name=None)</strong></td><br>  <td align="left">使用随机乱序的方法创建batches<br>tensors:用于入列的一个list或者dict<br>capacity:一个整数，表示队列中元素最大数目</td><br></tr><br><tr><br>  <td>tf.train.shuffle_batch_join(tensors_list, batch_size, <br>capacity, min_after_dequeue, seed=None, <br>enqueue_many=False, shapes=None, <br>allow_smaller_final_batch=False, <br>shared_name=None, name=None)</td><br>  <td align="left">随机乱序的tensors创建batches，<br>其中tensors_list参数为tensors元组或tensors字典的列表<br>len(tensors_list)个线程将启动，<br>线程i将tensors_list[i]的tensors入列<br>tensors_list[i1][j]与tensors_list[i2][j]有相同的类型和shape</td><br></tr><br></tbody></table><br><br><br><pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment"># 一个简单例子，使用tf.train.shuffle_batch创建一个具有32张图像和32个标签的batches.</span><br>image_batch, label_batch = tf.train.shuffle_batch(<br>      [single_image, single_label],<br>      batch_size=<span class="hljs-number">32</span>,<br>      num_threads=<span class="hljs-number">4</span>,<br>      capacity=<span class="hljs-number">50000</span>,<br>      min_after_dequeue=<span class="hljs-number">10000</span>)<br></code></pre><br><br><br><br><pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment">#Batching函数相关例子，以函数tf.train.shuffle_batch为例</span><br><span class="hljs-comment">#为training, evaluation等操作将样本batching，以下代码使用随机顺序打包样本</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">read_my_file_format</span><span class="hljs-params">(filename_queue)</span>:</span><br>  reader = tf.SomeReader()<br>  key, record_string = reader.read(filename_queue)<br>  example, label = tf.some_decoder(record_string)<br>  processed_example = some_processing(example)<br>  <span class="hljs-keyword">return</span> processed_example, label<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">input_pipeline</span><span class="hljs-params">(filenames, batch_size, num_epochs=None)</span>:</span><br>  filename_queue = tf.train.string_input_producer(<br>      filenames, num_epochs=num_epochs, shuffle=<span class="hljs-keyword">True</span>)<br>  example, label = read_my_file_format(filename_queue)<br>  <span class="hljs-comment"># min_after_dequeue defines how big a buffer we will randomly sample</span><br>  <span class="hljs-comment">#   from – bigger means better shuffling but slower start up and more</span><br>  <span class="hljs-comment">#   memory used.</span><br>  <span class="hljs-comment"># capacity must be larger than min_after_dequeue and the amount larger</span><br>  <span class="hljs-comment">#   determines the maximum we will prefetch.  Recommendation:</span><br>  <span class="hljs-comment">#   min_after_dequeue + (num_threads + a small safety margin) <em> batch_size</em></span><br>  min_after_dequeue = <span class="hljs-number">10000</span><br>  capacity = min_after_dequeue + <span class="hljs-number">3</span>  batch_size<br>  example_batch, label_batch = tf.train.shuffle_batch(<br>      [example, label], batch_size=batch_size, capacity=capacity,<br>      min_after_dequeue=min_after_dequeue)<br>  <span class="hljs-keyword">return</span> example_batch, label_batch</code></pre><br><br><br><br><pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment">#如果需要跟多的并行或文件之间的样本乱序操作，可以使用函数tf.train.shuffle_batch_join多实例化reader</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">read_my_file_format</span><span class="hljs-params">(filename_queue)</span>:</span><br>  <span class="hljs-comment"># 与上例子相同</span><br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">input_pipeline</span><span class="hljs-params">(filenames, batch_size, read_threads, num_epochs=None)</span>:</span><br>  filename_queue = tf.train.string_input_producer(<br>      filenames, num_epochs=num_epochs, shuffle=<span class="hljs-keyword">True</span>)<br>  example_list = [read_my_file_format(filename_queue)<br>                  <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(read_threads)]<br>  min_after_dequeue = <span class="hljs-number">10000</span><br>  capacity = min_after_dequeue + <span class="hljs-number">3</span> * batch_size<br>  example_batch, label_batch = tf.train.shuffle_batch_join(<br>      example_list, batch_size=batch_size, capacity=capacity,<br>      min_after_dequeue=min_after_dequeue)<br>  <span class="hljs-keyword">return</span> example_batch, label_batch<br></code></pre><br><br><hr><br><br><hr><br><br><p><strong>相关链接：</strong></p><br><br><p>[1] 安装Tensorflow（Linux ubuntu） <a href="http://blog.csdn.net/lenbow/article/details/51203526" target="_blank" rel="noopener">http://blog.csdn.net/lenbow/article/details/51203526</a> <br><br>[2] ubuntu下CUDA编译的GCC降级安装 <a href="http://blog.csdn.net/lenbow/article/details/51596706" target="_blank" rel="noopener">http://blog.csdn.net/lenbow/article/details/51596706</a> <br><br>[3] ubuntu手动安装最新Nvidia显卡驱动 <a href="http://blog.csdn.net/lenbow/article/details/51683783" target="_blank" rel="noopener">http://blog.csdn.net/lenbow/article/details/51683783</a> <br><br>[4] Tensorflow的CUDA升级，以及相关配置 <a href="http://blog.csdn.net/lenbow/article/details/52118116" target="_blank" rel="noopener">http://blog.csdn.net/lenbow/article/details/52118116</a> <br><br>[5] 基于gensim的Doc2Vec简析 <a href="http://blog.csdn.net/lenbow/article/details/52120230" target="_blank" rel="noopener">http://blog.csdn.net/lenbow/article/details/52120230</a> <br><br>[6] TensorFlow的分布式学习框架简介 <a href="http://blog.csdn.net/lenbow/article/details/52130565" target="_blank" rel="noopener">http://blog.csdn.net/lenbow/article/details/52130565</a> <br><br>[7] Tensorflow一些常用基本概念与函数（1）  <a href="http://blog.csdn.net/lenbow/article/details/52152766" target="_blank" rel="noopener">http://blog.csdn.net/lenbow/article/details/52152766</a></p></div>

<div id="article_content" class="article_content"><br>        <div class="markdown_views"><p>摘要：本系列主要对tf的一些常用概念与方法进行描述。本文主要针对tensorflow的数据IO、图的运行等相关函数进行讲解。为‘Tensorflow一些常用基本概念与函数’系列之三。</p><br><br><br><br><h2 id="1序言">1、序言</h2><br><br><p>本文所讲的内容主要为以下相关函数：</p><br><br><table><br><thead><br><tr><br>  <th>操作组</th><br>  <th align="left">操作</th><br></tr><br></thead><br><tbody><tr><br>  <td>Data IO (Python functions)</td><br>  <td align="left">TFRecordWrite，rtf_record_iterator</td><br></tr><br><tr><br>  <td>Running Graphs</td><br>  <td align="left">Session management，Error classes</td><br></tr><br></tbody></table><br><br><br><h2 id="2tf函数">2、tf函数</h2><br><br><br><br><h3 id="21-数据io-data-io-python-functions">2.1 数据IO {Data IO (Python functions)}</h3><br><br><p>一个TFRecords 文件为一个字符串序列。这种格式并非随机获取，它比较适合大规模的数据流，而不太适合需要快速分区或其他非序列获取方式。</p><br><br><br><br><h4 id="数据io-data-io-python-functions">数据IO {Data IO (Python functions)}</h4><br><br><table><br><thead><br><tr><br>  <th>操作</th><br>  <th align="left">描述</th><br></tr><br></thead><br><tbody><tr><br>  <td>class tf.python_io.TFRecordWriter</td><br>  <td align="left">一个用于将记录(records)写入TFRecords文件的类</td><br></tr><br><tr><br>  <td>tf.python_io.TFRecordWriter.<strong>init</strong>(path, options=None)</td><br>  <td align="left">打开文件路径，并创建一个TFRecordWriter以供写入</td><br></tr><br><tr><br>  <td>tf.python_io.TFRecordWriter.write(record)</td><br>  <td align="left">将一个字符串records写入文件中</td><br></tr><br><tr><br>  <td>tf.python_io.TFRecordWriter.close()</td><br>  <td align="left">关闭文件</td><br></tr><br><tr><br>  <td>tf.python_io.tf_record_iterator(path, options=None)</td><br>  <td align="left">从TFRecords文件中读取记录的迭代器</td><br></tr><br></tbody></table><br><br><br><hr><br><br><br><br><h3 id="22-运行图running-graphs">2.2 运行图(Running Graphs)</h3><br><br><br><br><h4 id="会话管理-session-management"><strong>会话管理 (Session management)</strong></h4><br><br><table><br><thead><br><tr><br>  <th>操作</th><br>  <th align="left">描述</th><br></tr><br></thead><br><tbody><tr><br>  <td>class tf.Session</td><br>  <td align="left">运行TF操作的类,<br>一个Session对象将操作节点op封装在一定的环境内运行，<br>同时tensor对象将被计算求值</td><br></tr><br><tr><br>  <td>tf.Session.<strong>init</strong>(target=”, graph=None, config=None)</td><br>  <td align="left">创建一个新的会话</td><br></tr><br><tr><br>  <td>tf.Session.run(fetches, feed_dict=None, <br>options=None, run_metadata=None)</td><br>  <td align="left">运行fetches中的操作节点并求其值</td><br></tr><br><tr><br>  <td>tf.Session.close()</td><br>  <td align="left">关闭会话</td><br></tr><br><tr><br>  <td>tf.Session.graph</td><br>  <td align="left">返回加载值该会话的图(graph)</td><br></tr><br><tr><br>  <td><strong>tf.Session.as_default()</strong></td><br>  <td align="left">设置该对象为默认会话，并返回一个上下文管理器</td><br></tr><br><tr><br>  <td>tf.Session.reset(target, containers=None, config=None)</td><br>  <td align="left">重设target的资源容器，并关闭所有连接的会话<br>在0.10版本该功能仅应用在分布会话中<br>target:为执行引擎所连接的目标，其包含有资源容器，<br>该资源容器分布在同一个集群的所有works上</td><br></tr><br><tr><br>  <td>class tf.InteractiveSession</td><br>  <td align="left">使用在交互式上下文环境的tf会话，比如shell，ipython</td><br></tr><br><tr><br>  <td>tf.InteractiveSession.close()</td><br>  <td align="left">关闭一个InteractiveSession</td><br></tr><br><tr><br>  <td>tf.get_default_session()</td><br>  <td align="left">返回当前线程的默认会话</td><br></tr><br></tbody></table><br><br><br><p><strong><em>tf.Session</em></strong></p><br><br><br><br><pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment">#一个简单的tf.Session例子</span><br><span class="hljs-comment"># 建立一个graph.</span><br>a = tf.constant(<span class="hljs-number">5.0</span>)<br>b = tf.constant(<span class="hljs-number">6.0</span>)<br>c = a <em> b<br><br><span class="hljs-comment"># 将graph载入到一个会话session中</span><br>sess = tf.Session()<br><br><span class="hljs-comment"># 计算tensor <code>c</code>.</span><br>print(sess.run(c))<br></em></code></pre><br><br><br><br><pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment">#一个会话可能会占用一些资源，比如变量、队列和读取器(reader)。释放这些不再使用的资源非常重要。</span><br><span class="hljs-comment">#使用close()方法关闭会话，或者使用上下文管理器，释放资源。</span><br><span class="hljs-comment"># 使用<code>close()</code>方法.</span><br>sess = tf.Session()<br>sess.run(…)<br>sess.close()<br><br><span class="hljs-comment"># 使用上下文管理器</span><br><span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:<br>  sess.run(…)<br></code></pre><br><br><p>tf.Session()的变量设置， ConfigProto protocol buffer为会话提供了不同的配置选项。比如，创建一个会话，对设备布局使用软约束条件，以及对分布</p><br><br><br><br><pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment"># Launch the graph in a session that allows soft device placement and</span><br><span class="hljs-comment"># logs the placement decisions.</span><br>sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=<span class="hljs-keyword">True</span>,<br>                                        log_device_placement=<span class="hljs-keyword">True</span>))<br></code></pre><br><br><p><strong><em>tf.Session.run</em></strong></p><br><br><br><br><pre class="prettyprint"><code class="language-python hljs "><br> a = tf.constant([<span class="hljs-number">10</span>, <span class="hljs-number">20</span>])<br>   b = tf.constant([<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>])<br>   <span class="hljs-comment"># ‘fetches’ 可以为单个数</span><br>   v = session.run(a)<br>   <span class="hljs-comment"># v is the numpy array [10, 20]</span><br>   <span class="hljs-comment"># ‘fetches’ 可以为一个list.</span><br>   v = session.run([a, b])<br>   <span class="hljs-comment"># v a Python list with 2 numpy arrays: the numpy array [10, 20] and the</span><br>   <span class="hljs-comment"># 1-D array [1.0, 2.0]</span><br>   <span class="hljs-comment"># ‘fetches’ 可以是 lists, tuples, namedtuple, dicts中的任意:</span><br>   MyData = collections.namedtuple(<span class="hljs-string">‘MyData’</span>, [<span class="hljs-string">‘a’</span>, <span class="hljs-string">‘b’</span>])<br>   v = session.run({<span class="hljs-string">‘k1’</span>: MyData(a, b), <span class="hljs-string">‘k2’</span>: [b, a]})<br>   <span class="hljs-comment"># v 为一个dict，并有</span><br>   <span class="hljs-comment"># v[‘k1’] is a MyData namedtuple with ‘a’ the numpy array [10, 20] and</span><br>   <span class="hljs-comment"># ‘b’ the numpy array [1.0, 2.0]</span><br>   <span class="hljs-comment"># v[‘k2’] is a list with the numpy array [1.0, 2.0] and the numpy array</span><br>   <span class="hljs-comment"># [10, 20].</span><br></code></pre><br><br><p><strong><em>tf.Session.as_default()</em></strong> <br><br>使用关键字with指定会话， 可以在会话中执行<a href="http://blog.csdn.net/lenbow/article/details/52181159" target="_blank" rel="noopener">Operation.run()</a>或<a href="http://blog.csdn.net/lenbow/article/details/52181159" target="_blank" rel="noopener">Tensor.eval()</a>，以得到运行的tensor结果</p><br><br><br><br><pre class="prettyprint"><code class="language-python hljs ">c = tf.constant(..)<br>sess = tf.Session()<br><br><span class="hljs-keyword">with</span> sess.as_default():<br>  <span class="hljs-keyword">assert</span> tf.get_default_session() <span class="hljs-keyword">is</span> sess<br>  print(c.eval())<br></code></pre><br><br><p>使用函数tf.get_default_session()来得到当前默认的会话 <br><br>需要注意的是，退出该as_default上下文管理器时，并没有关闭该会话(session )，必须明确的关闭会话</p><br><br><br><br><pre class="prettyprint"><code class="language-python hljs ">c = tf.constant(…)<br>sess = tf.Session()<br><span class="hljs-keyword">with</span> sess.as_default():<br>  print(c.eval())<br><span class="hljs-comment"># …</span><br><span class="hljs-keyword">with</span> sess.as_default():<br>  print(c.eval())<br><span class="hljs-comment">#关闭会话</span><br>sess.close()<br><span class="hljs-comment">#使用 with tf.Session()方式可以创建并自动关闭会话</span></code></pre><br><br><p><strong><em>tf.InteractiveSession</em></strong></p><br><br><br><br><pre class="prettyprint"><code class="language-python hljs ">sess = tf.InteractiveSession()<br>a = tf.constant(<span class="hljs-number">5.0</span>)<br>b = tf.constant(<span class="hljs-number">6.0</span>)<br>c = a  b<br><span class="hljs-comment"># 我们直接使用’c.eval()’ 而没有通过’sess’</span><br>print(c.eval())<br>sess.close()</code></pre><br><br><p>以上的例子，在非交互会话的版本中为，</p><br><br><br><br><pre class="prettyprint"><code class="language-python hljs ">a = tf.constant(<span class="hljs-number">5.0</span>)<br>b = tf.constant(<span class="hljs-number">6.0</span>)<br>c = a * b<br><span class="hljs-keyword">with</span> tf.Session():<br>  <span class="hljs-comment"># We can also use ‘c.eval()’ here.</span><br>  print(c.eval())</code></pre><br><br><br><br><h1 id="abc">ABC</h1><br><br><br><br><h4 id="错误类-error-classes"><strong>错误类 (Error classes)</strong></h4><br><br><table><br><thead><br><tr><br>  <th>操作</th><br>  <th align="left">描述</th><br></tr><br></thead><br><tbody><tr><br>  <td>class tf.OpError</td><br>  <td align="left">一个基本的错误类型，在当TF执行失败时候报错</td><br></tr><br><tr><br>  <td>tf.OpError.op</td><br>  <td align="left">返回执行失败的操作节点，<br>有的操作如Send或Recv可能不会返回，那就要用用到node_def方法</td><br></tr><br><tr><br>  <td><strong>tf.OpError.node_def</strong></td><br>  <td align="left">以NodeDef proto形式表示失败的op</td><br></tr><br><tr><br>  <td>tf.OpError.error_code</td><br>  <td align="left">描述该错误的整数错误代码</td><br></tr><br><tr><br>  <td>tf.OpError.message</td><br>  <td align="left">返回错误信息</td><br></tr><br><tr><br>  <td>class tf.errors.CancelledError</td><br>  <td align="left">当操作或者阶段呗取消时候报错</td><br></tr><br><tr><br>  <td>class tf.errors.UnknownError</td><br>  <td align="left">未知错误类型</td><br></tr><br><tr><br>  <td>class tf.errors.InvalidArgumentError</td><br>  <td align="left">在接收到非法参数时候报错</td><br></tr><br><tr><br>  <td>class tf.errors.NotFoundError</td><br>  <td align="left">当发现不存在所请求的一个实体时候，比如文件或目录</td><br></tr><br><tr><br>  <td>class tf.errors.AlreadyExistsError</td><br>  <td align="left">当创建的实体已经存在的时候报错</td><br></tr><br><tr><br>  <td>class tf.errors.PermissionDeniedError</td><br>  <td align="left">没有执行权限做某操作的时候报错</td><br></tr><br><tr><br>  <td>class tf.errors.ResourceExhaustedError</td><br>  <td align="left">资源耗尽时报错</td><br></tr><br><tr><br>  <td>class tf.errors.FailedPreconditionError</td><br>  <td align="left">系统没有条件执行某个行为时候报错</td><br></tr><br><tr><br>  <td>class tf.errors.AbortedError</td><br>  <td align="left">操作中止时报错，常常发生在并发情形</td><br></tr><br><tr><br>  <td>class tf.errors.OutOfRangeError</td><br>  <td align="left">超出范围报错</td><br></tr><br><tr><br>  <td>class tf.errors.UnimplementedError</td><br>  <td align="left">某个操作没有执行时报错</td><br></tr><br><tr><br>  <td>class tf.errors.InternalError</td><br>  <td align="left">当系统经历了一个内部错误时报出</td><br></tr><br><tr><br>  <td>class tf.errors.DataLossError</td><br>  <td align="left">当出现不可恢复的错误<br>例如在运行 <a href="http://blog.csdn.net/lenbow/article/details/52181159" target="_blank" rel="noopener">tf.WholeFileReader.read()</a>读取整个文件的同时文件被删减</td><br></tr><br><tr><br>  <td>tf.errors.XXXXX.<strong>init</strong>(node_def, op, message)</td><br>  <td align="left">使用该形式方法创建以上各种错误类</td><br></tr><br></tbody></table><br><br><br><hr><br><br><hr><br><br><p>相关链接：</p><br><br><p>[1] 安装Tensorflow（Linux ubuntu） <a href="http://blog.csdn.net/lenbow/article/details/51203526" target="_blank" rel="noopener">http://blog.csdn.net/lenbow/article/details/51203526</a> <br><br>[2] ubuntu下CUDA编译的GCC降级安装 <a href="http://blog.csdn.net/lenbow/article/details/51596706" target="_blank" rel="noopener">http://blog.csdn.net/lenbow/article/details/51596706</a> <br><br>[3] ubuntu手动安装最新Nvidia显卡驱动 <a href="http://blog.csdn.net/lenbow/article/details/51683783" target="_blank" rel="noopener">http://blog.csdn.net/lenbow/article/details/51683783</a> <br><br>[4] Tensorflow的CUDA升级，以及相关配置 <a href="http://blog.csdn.net/lenbow/article/details/52118116" target="_blank" rel="noopener">http://blog.csdn.net/lenbow/article/details/52118116</a> <br><br>[5] 基于gensim的Doc2Vec简析 <a href="http://blog.csdn.net/lenbow/article/details/52120230" target="_blank" rel="noopener">http://blog.csdn.net/lenbow/article/details/52120230</a> <br><br>[6] TensorFlow的分布式学习框架简介 <a href="http://blog.csdn.net/lenbow/article/details/52130565" target="_blank" rel="noopener">http://blog.csdn.net/lenbow/article/details/52130565</a> <br><br>[7] Tensorflow一些常用基本概念与函数（1） <a href="http://blog.csdn.net/lenbow/article/details/52152766" target="_blank" rel="noopener">http://blog.csdn.net/lenbow/article/details/52152766</a> <br><br>[8] Tensorflow一些常用基本概念与函数（2） <a href="http://blog.csdn.net/lenbow/article/details/52181159" target="_blank" rel="noopener">http://blog.csdn.net/lenbow/article/details/52181159</a></p></div>

<div id="article_content" class="article_content"><br>        <div class="markdown_views"><p>摘要：本系列主要对tf的一些常用概念与方法进行描述。本文主要针对tensorflow的模型训练Training与测试Testing等相关函数进行讲解。为‘Tensorflow一些常用基本概念与函数’系列之四。</p><br><br><h2 id="1序言">1、序言</h2><br><br><p>本文所讲的内容主要为以下列表中相关函数。函数training()通过梯度下降法为最小化损失函数增加了相关的优化操作，在训练过程中，先实例化一个优化函数，比如 tf.train.GradientDescentOptimizer，并基于一定的学习率进行梯度优化训练：</p><br><br><pre><code>optimizer = tf.train.GradientDescentOptimizer(learning_rate)<br></code></pre><br><br><p>然后，可以设置 一个用于记录全局训练步骤的单值。以及使用minimize()操作，该操作不仅可以优化更新训练的模型参数，也可以为全局步骤(global step)计数。与其他tensorflow操作类似，这些训练操作都需要在<a href="http://blog.csdn.net/lenbow/article/details/52181159" target="_blank" rel="noopener">tf.session</a>会话中进行</p><br><br><pre><code>global_step = tf.Variable(0, name=’global_step’, trainable=False)<br>train_op = optimizer.minimize(loss, global_step=global_step)<br></code></pre><br><br><table><br><thead><br><tr><br>  <th>操作组</th><br>  <th align="left">操作</th><br></tr><br></thead><br><tbody><tr><br>  <td>Training</td><br>  <td align="left">Optimizers，Gradient Computation，Gradient Clipping，Distributed execution</td><br></tr><br><tr><br>  <td>Testing</td><br>  <td align="left">Unit tests，Utilities，Gradient checking</td><br></tr><br></tbody></table><br><br><br><br><br><h2 id="2tensorflow函数">2、Tensorflow函数</h2><br><br><br><br><h3 id="21-训练-training">2.1 训练 (Training)</h3><br><br><p>一个TFRecords 文件为一个字符串序列。这种格式并非随机获取，它比较适合大规模的数据流，而不太适合需要快速分区或其他非序列获取方式。</p><br><br><br><br><h4 id="优化-optimizers">█ <strong>优化 (Optimizers)</strong></h4><br><br><p>tf中各种优化类提供了为损失函数计算梯度的方法，其中包含比较经典的优化算法，比如GradientDescent 和Adagrad。</p><br><br><p>▶▶<strong><em>class tf.train.Optimizer</em></strong></p><br><br><table><br><thead><br><tr><br>  <th>操作</th><br>  <th align="left">描述</th><br></tr><br></thead><br><tbody><tr><br>  <td>class tf.train.Optimizer</td><br>  <td align="left">基本的优化类，该类不常常被直接调用，而较多使用其子类，<br>比如GradientDescentOptimizer, AdagradOptimizer<br>或者MomentumOptimizer</td><br></tr><br><tr><br>  <td>tf.train.Optimizer.<strong>init</strong>(use_locking, name)</td><br>  <td align="left">创建一个新的优化器，<br>该优化器必须被其子类(subclasses)的构造函数调用</td><br></tr><br><tr><br>  <td><strong>tf.train.Optimizer.minimize(loss, global_step=None, <br>var_list=None, gate_gradients=1, <br>aggregation_method=None, colocate_gradients_with_ops=False, <br>name=None, grad_loss=None)</strong></td><br>  <td align="left">添加操作节点，用于最小化loss，并更新var_list<br>该函数是简单的合并了compute_gradients()与apply_gradients()函数<br>返回为一个优化更新后的var_list，如果global_step非None，该操作还会为global_step做自增操作</td><br></tr><br><tr><br>  <td>tf.train.Optimizer.compute_gradients(loss,var_list=None, gate_gradients=1,<br> aggregation_method=None, <br>colocate_gradients_with_ops=False, grad_loss=None)</td><br>  <td align="left">对var_list中的变量计算loss的梯度<br>该函数为函数minimize()的第一部分，返回一个以元组(gradient, variable)组成的列表</td><br></tr><br><tr><br>  <td>tf.train.Optimizer.apply_gradients(grads_and_vars, global_step=None, name=None)</td><br>  <td align="left">将计算出的梯度应用到变量上，是函数minimize()的第二部分，返回一个应用指定的梯度的操作Operation，对global_step做自增操作</td><br></tr><br><tr><br>  <td>tf.train.Optimizer.get_name()</td><br>  <td align="left">获取名称</td><br></tr><br></tbody></table><br><br><br><p>▷ <em>class tf.train.Optimizer</em> <br><br>用法</p><br><br><br><br><pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment"># Create an optimizer with the desired parameters.</span><br>opt = GradientDescentOptimizer(learning_rate=<span class="hljs-number">0.1</span>)<br><span class="hljs-comment"># Add Ops to the graph to minimize a cost by updating a list of variables.</span><br><span class="hljs-comment"># “cost” is a Tensor, and the list of variables contains tf.Variable objects.</span><br>opt_op = opt.minimize(cost, var_list=&lt;list of variables&gt;)<br><span class="hljs-comment"># Execute opt_op to do one step of training:</span><br>opt_op.run()</code></pre><br><br><p>▶▶<strong><em>在使用它们之前处理梯度</em></strong> <br><br>使用minimize()操作，该操作不仅可以计算出梯度，而且还可以将梯度作用在变量上。如果想在使用它们之前处理梯度，可以按照以下三步骤使用optimizer ：</p><br><br><pre><code>1、使用函数compute_gradients()计算梯度<br>2、按照自己的愿望处理梯度<br>3、使用函数apply_gradients()应用处理过后的梯度<br></code></pre><br><br><p>例如：</p><br><br><br><br><pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment"># 创建一个optimizer.</span><br>opt = GradientDescentOptimizer(learning_rate=<span class="hljs-number">0.1</span>)<br><br><span class="hljs-comment"># 计算&lt;list of variables&gt;相关的梯度</span><br>grads_and_vars = opt.compute_gradients(loss, &lt;list of variables&gt;)<br><br><span class="hljs-comment"># grads_and_vars为tuples (gradient, variable)组成的列表。</span><br><span class="hljs-comment">#对梯度进行想要的处理，比如cap处理</span><br>capped_grads_and_vars = [(MyCapper(gv[<span class="hljs-number">0</span>]), gv[<span class="hljs-number">1</span>]) <span class="hljs-keyword">for</span> gv <span class="hljs-keyword">in</span> grads_and_vars]<br><br><span class="hljs-comment"># 令optimizer运用capped的梯度(gradients)</span><br>opt.apply_gradients(capped_grads_and_vars)</code></pre><br><br><p>▶▶<strong><em>选通梯度(Gating Gradients)</em></strong> <br><br>函数minimize() 与compute_gradients()都含有一个参数gate_gradient，用于控制在应用这些梯度时并行化的程度。</p><br><br><p>其值可以取：GATE_NONE, GATE_OP 或 GATE_GRAPH <br><br>GATE_NONE : 并行地计算和应用梯度。提供最大化的并行执行，但是会导致有的数据结果没有再现性。比如两个matmul操作的梯度依赖输入值，使用GATE_NONE可能会出现有一个梯度在其他梯度之前便应用到某个输入中，导致出现不可再现的(non-reproducible)结果 <br><br>GATE_OP: 对于每个操作Op，确保每一个梯度在使用之前都已经计算完成。这种做法防止了那些具有多个输入，并且梯度计算依赖输入情形中，多输入Ops之间的竞争情况出现。 <br><br> GATE_GRAPH: 确保所有的变量对应的所有梯度在他们任何一个被使用前计算完成。该方式具有最低级别的并行化程度，但是对于想要在应用它们任何一个之前处理完所有的梯度计算时很有帮助的。</p><br><br><p><br></p><br><br><br><br><h4 id="slots">█ <strong>Slots</strong></h4><br><br><p>一些optimizer的之类，比如 MomentumOptimizer 和 AdagradOptimizer 分配和管理着额外的用于训练的变量。这些变量称之为’Slots’，Slots有相应的名称，可以向optimizer访问的slots名称。有助于在log debug一个训练算法以及报告slots状态</p><br><br><table><br><thead><br><tr><br>  <th>操作</th><br>  <th align="left">描述</th><br></tr><br></thead><br><tbody><tr><br>  <td>tf.train.Optimizer.get_slot_names()</td><br>  <td align="left">返回一个由Optimizer所创建的slots的名称列表</td><br></tr><br><tr><br>  <td>tf.train.Optimizer.get_slot(var, name)</td><br>  <td align="left">返回一个name所对应的slot，name是由Optimizer为var所创建<br>var为用于传入 minimize() 或 apply_gradients()的变量</td><br></tr><br><tr><br>  <td><strong>class tf.train.GradientDescentOptimizer</strong></td><br>  <td align="left">使用梯度下降算法的Optimizer</td><br></tr><br><tr><br>  <td>tf.train.GradientDescentOptimizer.<strong>init</strong>(learning_rate, <br>use_locking=False, name=’GradientDescent’)</td><br>  <td align="left">构建一个新的梯度下降优化器(Optimizer)</td><br></tr><br><tr><br>  <td><strong>class tf.train.AdadeltaOptimizer</strong></td><br>  <td align="left">使用<a href="http://arxiv.org/abs/1212.5701" target="_blank" rel="noopener">Adadelta算法</a>的Optimizer</td><br></tr><br><tr><br>  <td>tf.train.AdadeltaOptimizer.<strong>init</strong>(learning_rate=0.001, <br>rho=0.95, epsilon=1e-08, <br>use_locking=False, name=’Adadelta’)</td><br>  <td align="left">创建Adadelta优化器</td><br></tr><br><tr><br>  <td><strong>class tf.train.AdagradOptimizer</strong></td><br>  <td align="left">使用<a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf" target="_blank" rel="noopener">Adagrad算法</a>的Optimizer</td><br></tr><br><tr><br>  <td>tf.train.AdagradOptimizer.<strong>init</strong>(learning_rate, <br>initial_accumulator_value=0.1, <br>use_locking=False, name=’Adagrad’)</td><br>  <td align="left">创建Adagrad优化器</td><br></tr><br><tr><br>  <td>class tf.train.MomentumOptimizer</td><br>  <td align="left">使用<a href="http://www.cs.toronto.edu/%7Etijmen/csc321/slides/lecture_slides_lec6.pdf" target="_blank" rel="noopener">Momentum</a>算法的Optimizer</td><br></tr><br><tr><br>  <td>tf.train.MomentumOptimizer.<strong>init</strong>(learning_rate, <br>momentum, use_locking=False, <br>name=’Momentum’, use_nesterov=False)</td><br>  <td align="left">创建momentum优化器<br>momentum：动量，一个tensor或者浮点值</td><br></tr><br><tr><br>  <td><strong>class tf.train.AdamOptimizer</strong></td><br>  <td align="left">使用<a href="http://arxiv.org/abs/1412.6980" target="_blank" rel="noopener">Adam 算法</a>的Optimizer</td><br></tr><br><tr><br>  <td>tf.train.AdamOptimizer.<strong>init</strong>(learning_rate=0.001,<br> beta1=0.9, beta2=0.999, epsilon=1e-08,<br> use_locking=False, name=’Adam’)</td><br>  <td align="left">创建Adam优化器</td><br></tr><br><tr><br>  <td>class tf.train.FtrlOptimizer</td><br>  <td align="left">使用<a href="https://www.eecs.tufts.edu/%7Edsculley/papers/ad-click-prediction.pdf" target="_blank" rel="noopener">FTRL 算法</a>的Optimizer</td><br></tr><br><tr><br>  <td>tf.train.FtrlOptimizer.<strong>init</strong>(learning_rate, <br>learning_rate_power=-0.5, <br>initial_accumulator_value=0.1, <br>l1_regularization_strength=0.0, <br>l2_regularization_strength=0.0,<br> use_locking=False, name=’Ftrl’)</td><br>  <td align="left">创建FTRL算法优化器</td><br></tr><br><tr><br>  <td>class tf.train.RMSPropOptimizer</td><br>  <td align="left">使用<a href="http://www.cs.toronto.edu/%7Etijmen/csc321/slides/lecture_slides_lec6.pdf" target="_blank" rel="noopener">RMSProp算法</a>的Optimizer</td><br></tr><br><tr><br>  <td>tf.train.RMSPropOptimizer.<strong>init</strong>(learning_rate, <br>decay=0.9, momentum=0.0, epsilon=1e-10, <br>use_locking=False, name=’RMSProp’)</td><br>  <td align="left">创建RMSProp算法优化器</td><br></tr><br></tbody></table><br><br><br><p>▷ <em><a href="http://arxiv.org/pdf/1412.6980.pdf" target="_blank" rel="noopener">tf.train.AdamOptimizer</a></em> <br><br>Adam 的基本运行方式，首先初始化：</p><br><br><pre><code>m_0 &lt;- 0 (Initialize initial 1st moment vector)<br>v_0 &lt;- 0 (Initialize initial 2nd moment vector)<br>t &lt;- 0 (Initialize timestep)<br></code></pre><br><br><p>在<a href="http://arxiv.org/pdf/1412.6980.pdf" target="_blank" rel="noopener">论文</a>中的 section2 的末尾所描述了更新规则，该规则使用梯度g来更新变量：</p><br><br><pre><code>t &lt;- t + 1<br>lr_t &lt;- learning_rate <em> sqrt(1 - beta2^t) / (1 - beta1^t)<br><br>m_t &lt;- beta1 </em> m_{t-1} + (1 - beta1) <em> g<br>v_t &lt;- beta2 </em> v_{t-1} + (1 - beta2) <em> g </em> g<br>variable &lt;- variable - lr_t <em> m_t / (sqrt(v_t) + epsilon)<br></em></code></pre><br><br><p>其中epsilon 的默认值1e-8可能对于大多数情况都不是一个合适的值。例如，当在ImageNet上训练一个 Inception network时比较好的选择为1.0或者0.1。 <br><br>需要注意的是，在稠密数据中即便g为0时， m_t, v_t 以及variable都将会更新。而在稀疏数据中，m_t, v_t 以及variable不被更新且值为零。</p><br><br><br><br><h4 id="梯度计算与截断gradient-computation-and-clipping">█ <strong>梯度计算与截断(Gradient Computation and Clipping)</strong></h4><br><br><p>TensorFlow 提供了计算给定tf计算图的求导函数，并在图的基础上增加节点。优化器(optimizer )类可以自动的计算网络图的导数，但是优化器中的创建器(creators )或者专业的人员可以通过本节所述的函数调用更底层的方法。</p><br><br><table><br><thead><br><tr><br>  <th>操作</th><br>  <th align="left">描述</th><br></tr><br></thead><br><tbody><tr><br>  <td>tf.gradients(ys, xs, grad_ys=None, name=’gradients’, <br>colocate_gradients_with_ops=False, gate_gradients=False, <br>aggregation_method=None)</td><br>  <td align="left">构建一个符号函数，计算ys关于xs中x的偏导的和，<br>返回xs中每个x对应的sum(dy/dx)</td><br></tr><br><tr><br>  <td>tf.stop_gradient(input, name=None)</td><br>  <td align="left">停止计算梯度，<br>在EM算法、Boltzmann机等可能会使用到</td><br></tr><br><tr><br>  <td>tf.clip_by_value(t, clip_value_min, clip_value_max, name=None)</td><br>  <td align="left">基于定义的min与max对tesor数据进行截断操作，<br>目的是为了应对梯度爆发或者梯度消失的情况</td><br></tr><br><tr><br>  <td>tf.clip_by_norm(t, clip_norm, axes=None, name=None)</td><br>  <td align="left">使用L2范式标准化tensor最大值为clip_norm<br>返回 t  clip_norm / l2norm(t)</td><br></tr><br><tr><br>  <td>tf.clip_by_average_norm(t, clip_norm, name=None)</td><br>  <td align="left">使用平均L2范式规范tensor数据t，<br>并以clip_norm为最大值<br>返回 t <em> clip_norm / l2norm_avg(t)</em></td><br></tr><br><tr><br>  <td>tf.clip_by_global_norm(t_list, <br>clip_norm, use_norm=None, name=None)</td><br>  <td align="left">返回t_list[i]  clip_norm / max(global_norm, clip_norm)<br>其中global_norm = sqrt(sum([l2norm(t)<strong>2 for t in t_list]))</strong></td><br></tr><br><tr><br>  <td>tf.global_norm(t_list, name=None)</td><br>  <td align="left">返回global_norm = sqrt(sum([l2norm(t)2 for t in t_list]))</td><br></tr><br></tbody></table><br><br><br><p><br></p><br><br><br><br><h4 id="退化学习率decaying-the-learning-rate">█ <strong>退化学习率(Decaying the learning rate)</strong></h4><br><br><table><br><thead><br><tr><br>  <th>操作</th><br>  <th align="left">描述</th><br></tr><br></thead><br><tbody><tr><br>  <td>tf.train.exponential_decay(learning_rate, global_step, <br>decay_steps, decay_rate, staircase=False, name=None)</td><br>  <td align="left">对学习率进行指数衰退</td><br></tr><br></tbody></table><br><br><br><p>▷ <em>tf.train.exponential_decay</em></p><br><br><br><br><pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment">#该函数返回以下结果</span><br>decayed_learning_rate = learning_rate <em><br>         decay_rate ^ (global_step / decay_steps)<br><span class="hljs-comment">##例： 以0.96为基数，每100000 步进行一次学习率的衰退</span><br>global_step = tf.Variable(<span class="hljs-number">0</span>, trainable=<span class="hljs-keyword">False</span>)<br>starter_learning_rate = <span class="hljs-number">0.1</span><br>learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,<br>                                           <span class="hljs-number">100000</span>, <span class="hljs-number">0.96</span>, staircase=<span class="hljs-keyword">True</span>)<br><span class="hljs-comment"># Passing global_step to minimize() will increment it at each step.</span><br>learning_step = (<br>    tf.train.GradientDescentOptimizer(learning_rate)<br>    .minimize(…my loss…, global_step=global_step)<br>)<br></em></code></pre><br><br><p><br></p><br><br><br><br><h4 id="移动平均moving-averages">█ <strong>移动平均(Moving Averages)</strong></h4><br><br><p>一些训练优化算法，比如GradientDescent 和Momentum 在优化过程中便可以使用到移动平均方法。使用移动平均常常可以较明显地改善结果。</p><br><br><table><br><thead><br><tr><br>  <th>操作</th><br>  <th align="left">描述</th><br></tr><br></thead><br><tbody><tr><br>  <td>class tf.train.ExponentialMovingAverage</td><br>  <td align="left">将指数衰退加入到移动平均中</td><br></tr><br><tr><br>  <td>tf.train.ExponentialMovingAverage.apply(var_list=None)</td><br>  <td align="left">对var_list变量保持移动平均</td><br></tr><br><tr><br>  <td>tf.train.ExponentialMovingAverage.average_name(var)</td><br>  <td align="left">返回var均值的变量名称</td><br></tr><br><tr><br>  <td>tf.train.ExponentialMovingAverage.average(var)</td><br>  <td align="left">返回var均值变量</td><br></tr><br><tr><br>  <td>tf.train.ExponentialMovingAverage.variables_to_restore(moving_avg_variables=None)</td><br>  <td align="left">返回用于保存的变量名称的映射</td><br></tr><br></tbody></table><br><br><br><p>▷ <em>tf.train.ExponentialMovingAverage</em></p><br><br><br><br><pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment"># Example usage when creating a training model:</span><br><span class="hljs-comment"># Create variables.</span><br>var0 = tf.Variable(…)<br>var1 = tf.Variable(…)<br><span class="hljs-comment"># … use the variables to build a training model…</span><br>…<br><span class="hljs-comment"># Create an op that applies the optimizer.  This is what we usually</span><br><span class="hljs-comment"># would use as a training op.</span><br>opt_op = opt.minimize(my_loss, [var0, var1])<br><br><span class="hljs-comment"># Create an ExponentialMovingAverage object</span><br>ema = tf.train.ExponentialMovingAverage(decay=<span class="hljs-number">0.9999</span>)<br><br><span class="hljs-comment"># Create the shadow variables, and add ops to maintain moving averages</span><br><span class="hljs-comment"># of var0 and var1.</span><br>maintain_averages_op = ema.apply([var0, var1])<br><br><span class="hljs-comment"># Create an op that will update the moving averages after each training</span><br><span class="hljs-comment"># step.  This is what we will use in place of the usual training op.</span><br><span class="hljs-keyword">with</span> tf.control_dependencies([opt_op]):<br>    training_op = tf.group(maintain_averages_op)<br><br>…train the model by running training_op…<br><br><span class="hljs-comment">#Example of restoring the shadow variable values:</span><br><span class="hljs-comment"># Create a Saver that loads variables from their saved shadow values.</span><br>shadow_var0_name = ema.average_name(var0)<br>shadow_var1_name = ema.average_name(var1)<br>saver = tf.train.Saver({shadow_var0_name: var0, shadow_var1_name: var1})<br>saver.restore(…checkpoint filename…)<br><span class="hljs-comment"># var0 and var1 now hold the moving average values</span><br></code></pre><br><br><p>▷ <em>tf.train.ExponentialMovingAverage.variables_to_restore</em></p><br><br><br><br><pre class="prettyprint"><code class="language-python hljs "><br>  variables_to_restore = ema.variables_to_restore()<br>  saver = tf.train.Saver(variables_to_restore)<br></code></pre><br><br><p><br></p><br><br><br><br><h4 id="协调器和队列运行器coordinator-and-queuerunner">█ <strong>协调器和队列运行器(Coordinator and QueueRunner)</strong></h4><br><br><p>查看<a href="http://blog.csdn.net/lenbow/article/details/52181159" target="_blank" rel="noopener">queue</a>中，queue相关的内容，了解tensorflow中队列的运行方式。</p><br><br><table><br><thead><br><tr><br>  <th>操作</th><br>  <th align="left">描述</th><br></tr><br></thead><br><tbody><tr><br>  <td>class tf.train.Coordinator</td><br>  <td align="left">线程的协调器</td><br></tr><br><tr><br>  <td>tf.train.Coordinator.clear_stop()</td><br>  <td align="left">清除停止标记</td><br></tr><br><tr><br>  <td>tf.train.Coordinator.join(threads=None, stop_grace_period_secs=120)</td><br>  <td align="left">等待线程终止<br>threads:一个threading.Threads的列表，启动的线程，将额外加入到registered的线程中</td><br></tr><br><tr><br>  <td>tf.train.Coordinator.register_thread(thread)</td><br>  <td align="left">Register一个用于join的线程</td><br></tr><br><tr><br>  <td>tf.train.Coordinator.request_stop(ex=None)</td><br>  <td align="left">请求线程结束</td><br></tr><br><tr><br>  <td>tf.train.Coordinator.should_stop()</td><br>  <td align="left">检查是否被请求停止</td><br></tr><br><tr><br>  <td>tf.train.Coordinator.stop_on_exception()</td><br>  <td align="left">上下文管理器，当一个例外出现时请求停止</td><br></tr><br><tr><br>  <td>tf.train.Coordinator.wait_for_stop(timeout=None)</td><br>  <td align="left">等待Coordinator提示停止进程</td><br></tr><br><tr><br>  <td><strong>class tf.train.QueueRunner</strong></td><br>  <td align="left">持有一个队列的入列操作列表，用于线程中运行<br>queue:一个队列<br>enqueue_ops: 用于线程中运行的入列操作列表</td><br></tr><br><tr><br>  <td>tf.train.QueueRunner.create_threads(sess, <br>coord=None, daemon=False, start=False)</td><br>  <td align="left">创建运行入列操作的线程，返回一个线程列表</td><br></tr><br><tr><br>  <td>tf.train.QueueRunner.from_proto(queue_runner_def)</td><br>  <td align="left">返回由queue_runner_def创建的QueueRunner对象</td><br></tr><br><tr><br>  <td>tf.train.add_queue_runner(qr, collection=’queue_runners’)</td><br>  <td align="left">增加一个QueueRunner到graph的收集器(collection )中</td><br></tr><br><tr><br>  <td>tf.train.start_queue_runners(sess=None, coord=None, daemon=True, start=True, collection=’queue_runners’)</td><br>  <td align="left">启动所有graph收集到的队列运行器(queue runners)</td><br></tr><br></tbody></table><br><br><br><p>▷ <em>class tf.train.Coordinator</em></p><br><br><br><br><pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment">#Coordinator的使用，用于多线程的协调</span><br><span class="hljs-keyword">try</span>:<br>  …<br>  coord = Coordinator()<br>  <span class="hljs-comment"># Start a number of threads, passing the coordinator to each of them.</span><br>  …start thread <span class="hljs-number">1.</span>..(coord, …)<br>  …start thread N…(coord, …)<br>  <span class="hljs-comment"># Wait for all the threads to terminate, give them 10s grace period</span><br>  coord.join(threads, stop_grace_period_secs=<span class="hljs-number">10</span>)<br><span class="hljs-keyword">except</span> RuntimeException:<br>  …one of the threads took more than <span class="hljs-number">10</span>s to stop after request_stop()<br>  …was called.<br><span class="hljs-keyword">except</span> Exception:<br>  …exception that was passed to coord.request_stop()<br></code></pre><br><br><p>▷ <em>tf.train.Coordinator.stop_on_exception()</em></p><br><br><br><br><pre class="prettyprint"><code class="language-python hljs "><span class="hljs-keyword">with</span> coord.stop_on_exception():<br>  <span class="hljs-comment"># Any exception raised in the body of the with</span><br>  <span class="hljs-comment"># clause is reported to the coordinator before terminating</span><br>  <span class="hljs-comment"># the execution of the body.</span><br>  …body…<br><span class="hljs-comment">#等价于</span><br><span class="hljs-keyword">try</span>:<br>  …body…<br>exception Exception <span class="hljs-keyword">as</span> ex:<br>  coord.request_stop(ex)</code></pre><br><br><p><br></p><br><br><br><br><h4 id="布执行distributed-execution">█ <strong>布执行(Distributed execution)</strong></h4><br><br><p>可以阅读<a href="http://blog.csdn.net/lenbow/article/details/52130565" target="_blank" rel="noopener">TensorFlow的分布式学习框架简介 </a>查看更多tensorflow分布式细节。</p><br><br><table><br><thead><br><tr><br>  <th>操作</th><br>  <th align="left">描述</th><br></tr><br></thead><br><tbody><tr><br>  <td><strong>class tf.train.Server</strong></td><br>  <td align="left">一个进程内的tensorflow服务，用于分布式训练</td><br></tr><br><tr><br>  <td>tf.train.Server.<strong>init</strong>(server_or_cluster_def, <br>job_name=None, task_index=None, protocol=None,<br> config=None, start=True)</td><br>  <td align="left">创建一个新的服务，其中job_name, task_index, <br>和protocol为可选参数，<br>优先级高于server_or_cluster_def中相关信息<br>server_or_cluster_def :  为一个tf.train.ServerDef <br>或 tf.train.ClusterDef 协议(protocol)的buffer，<br>或者一个tf.train.ClusterSpec对象</td><br></tr><br><tr><br>  <td><strong>tf.train.Server.create_local_server(config=None, start=True)</strong></td><br>  <td align="left">创建一个新的运行在本地主机的单进程集群</td><br></tr><br><tr><br>  <td><strong>tf.train.Server.target</strong></td><br>  <td align="left">返回tf.Session所连接的目标服务器</td><br></tr><br><tr><br>  <td>tf.train.Server.server_def</td><br>  <td align="left">返回该服务的tf.train.ServerDef</td><br></tr><br><tr><br>  <td><strong>tf.train.Server.start()</strong></td><br>  <td align="left">开启服务</td><br></tr><br><tr><br>  <td>tf.train.Server.join()</td><br>  <td align="left">阻塞直到服务已经关闭</td><br></tr><br></tbody></table><br><br><br><table><br><thead><br><tr><br>  <th>#</th><br>  <th align="left"></th><br></tr><br></thead><br><tbody><tr><br>  <td>class tf.train.Supervisor</td><br>  <td align="left">一个训练辅助器，用于checkpoints模型以及计算的summaries。该监视器只是一个小的外壳(wrapper),用于Coordinator, a Saver, 和a SessionManager周围</td><br></tr><br><tr><br>  <td><strong>tf.train.Supervisor.<strong>init</strong>(graph=None, ready_op=0, is_chief=True, init_op=0, init_feed_dict=None, local_init_op=0, logdir=None, <br>summary_op=0, saver=0, global_step=0, <br>save_summaries_secs=120, save_model_secs=600, <br>recovery_wait_secs=30, stop_grace_secs=120,<br> checkpoint_basename=’model.ckpt’, session_manager=None, summary_writer=0, init_fn=None)</strong></td><br>  <td align="left">创建一个监视器Supervisor</td><br></tr><br><tr><br>  <td>tf.train.Supervisor.managed_session(master=”, config=None, start_standard_services=True, close_summary_writer=True)</td><br>  <td align="left">返回一个管路session的上下文管理器</td><br></tr><br><tr><br>  <td>tf.train.Supervisor.prepare_or_wait_for_session(master=”, config=None, wait_for_checkpoint=False, max_wait_secs=7200, start_standard_services=True)</td><br>  <td align="left">确保model已经准备好</td><br></tr><br><tr><br>  <td><strong>tf.train.Supervisor.start_standard_services(sess)</strong></td><br>  <td align="left">为sess启动一个标准的服务</td><br></tr><br><tr><br>  <td>tf.train.Supervisor.start_queue_runners(sess, queue_runners=None)</td><br>  <td align="left">为QueueRunners启动一个线程，queue_runners为一个QueueRunners列表</td><br></tr><br><tr><br>  <td>tf.train.Supervisor.summary_computed(sess, summary, global_step=None)</td><br>  <td align="left">指示计算的summary</td><br></tr><br><tr><br>  <td>tf.train.Supervisor.stop(threads=None, close_summary_writer=True)</td><br>  <td align="left">停止服务以及协调器(coordinator),并没有关闭session</td><br></tr><br><tr><br>  <td>tf.train.Supervisor.request_stop(ex=None)</td><br>  <td align="left">参考Coordinator.request_stop()</td><br></tr><br><tr><br>  <td>tf.train.Supervisor.should_stop()</td><br>  <td align="left">参考Coordinator.should_stop()</td><br></tr><br><tr><br>  <td>tf.train.Supervisor.stop_on_exception()</td><br>  <td align="left">参考 Coordinator.stop_on_exception()</td><br></tr><br><tr><br>  <td>tf.train.Supervisor.Loop(timer_interval_secs, target, args=None, kwargs=None)</td><br>  <td align="left">开启一个循环器线程用于调用一个函数<br>每经过timer_interval_secs秒执行，target(args, *<em>kwargs)</em></td><br></tr><br><tr><br>  <td>tf.train.Supervisor.coord</td><br>  <td align="left">返回监督器(Supervisor)使用的协调器(Coordinator )</td><br></tr><br></tbody></table><br><br><br><table><br><thead><br><tr><br>  <th>#</th><br>  <th align="left"></th><br></tr><br></thead><br><tbody><tr><br>  <td>class tf.train.SessionManager</td><br>  <td align="left">训练的辅助器，用于从checkpoint恢复数据以及创建一个session</td><br></tr><br><tr><br>  <td>tf.train.SessionManager.<strong>init</strong>(local_init_op=None, ready_op=None, graph=None, recovery_wait_secs=30)</td><br>  <td align="left">创建一个SessionManager</td><br></tr><br><tr><br>  <td>tf.train.SessionManager.prepare_session(master, init_op=None, saver=None, checkpoint_dir=None, wait_for_checkpoint=False, max_wait_secs=7200, config=None, init_feed_dict=None, init_fn=None)</td><br>  <td align="left">创建一个session，并确保model可以被使用</td><br></tr><br><tr><br>  <td>tf.train.SessionManager.recover_session(master, saver=None, checkpoint_dir=None, wait_for_checkpoint=False, max_wait_secs=7200, config=None)</td><br>  <td align="left">创建一个session，如果可以的话，使用恢复方法创建</td><br></tr><br><tr><br>  <td>tf.train.SessionManager.wait_for_session(master, config=None, max_wait_secs=inf)</td><br>  <td align="left">创建一个session，并等待model准备完成</td><br></tr><br></tbody></table><br><br><br><table><br><thead><br><tr><br>  <th>#</th><br>  <th align="left"></th><br></tr><br></thead><br><tbody><tr><br>  <td><strong>class tf.train.ClusterSpec</strong></td><br>  <td align="left">将一个集群表示为一系列“tasks”，并整合至“jobs”中</td><br></tr><br><tr><br>  <td>tf.train.ClusterSpec.as_cluster_def()</td><br>  <td align="left">返回该cluster中一个tf.train.ClusterDef协议的buffer</td><br></tr><br><tr><br>  <td>tf.train.ClusterSpec.as_dict()</td><br>  <td align="left">返回一个字典，由job名称对应于网络地址</td><br></tr><br><tr><br>  <td>tf.train.ClusterSpec.job_tasks(job_name)</td><br>  <td align="left">返回一个给定的job对应的task列表</td><br></tr><br><tr><br>  <td>tf.train.ClusterSpec.jobs</td><br>  <td align="left">返回该cluster的job名称列表</td><br></tr><br><tr><br>  <td><strong>tf.train.replica_device_setter(ps_tasks=0, ps_device=’/job:ps’, worker_device=’/job:worker’, merge_devices=True, cluster=None, ps_ops=None)</strong></td><br>  <td align="left">返回一个设备函数(device function)，以在建立一个副本graph的时候使用，设备函数(device function)用在with tf.device(device_function)中</td><br></tr><br></tbody></table><br><br><br><p>▷ <em>tf.train.Server</em></p><br><br><br><br><pre class="prettyprint"><code class="language-python hljs ">server = tf.train.Server(…)<br><span class="hljs-keyword">with</span> tf.Session(server.target):<br>  <span class="hljs-comment"># …</span><br></code></pre><br><br><p>▷ <em>tf.train.Supervisor</em> <br><br>相关参数： <br><br>ready_op : 一维 字符串 tensor。该tensor是用过监视器在prepare_or_wait_for_session()计算，检查model是否准备好可以使用。如果准备好，将返回一个空阵列，如果为None，该model没有被检查。 <br><br>is_chief : 如果为True，创建一个主监视器用于负责初始化与模型的恢复，若为False，则依赖主监视器。 <br><br>init_op : 一个操作，用于模型不能恢复时的初始化操作。默认初始化所有操作 <br><br>local_init_op : 可被所有监视器运行的初始化操作。 <br><br>logdir : 设置log目录 <br><br>summary_op : 一个操作(Operation )，返回Summary 和事件logs，需要设置 logdir <br><br>saver : 一个Saver对象 <br><br>save_summaries_secs : 保存summaries的间隔秒数 <br><br>save_model_secs : 保存model的间隔秒数 <br><br>checkpoint_basename :  checkpoint保存的基本名称</p><br><br><ul><br><li>使用在单进程中</li><br></ul><br><br><br><br><pre class="prettyprint"><code class="language-python hljs "><span class="hljs-keyword">with</span> tf.Graph().as_default():<br>  …add operations to the graph…<br>  <span class="hljs-comment"># Create a Supervisor that will checkpoint the model in ‘/tmp/mydir’.</span><br>  sv = Supervisor(logdir=<span class="hljs-string">‘/tmp/mydir’</span>)<br>  <span class="hljs-comment"># Get a TensorFlow session managed by the supervisor.</span><br>  <span class="hljs-keyword">with</span> sv.managed_session(FLAGS.master) <span class="hljs-keyword">as</span> sess:<br>    <span class="hljs-comment"># Use the session to train the graph.</span><br>    <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> sv.should_stop():<br>      sess.run(&lt;my_train_op&gt;)<br><span class="hljs-comment"># 在上下文管理器with sv.managed_session()内，所有在graph的变量都被初始化。</span><br><span class="hljs-comment"># 或者说，一些服务器checkpoint相应模型并增加summaries至事件log中。</span><br><span class="hljs-comment"># 如果有例外发生，should_stop()将返回True</span></code></pre><br><br><ul><br><li>使用在多副本运行情况中 <br><br>要使用副本训练已经部署在集群上的相同程序，必须指定其中一个task为主要，该task处理 initialization, checkpoints, summaries, 和recovery相关事物。其他task依赖该task。</li><br></ul><br><br><br><br><pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment"># Choose a task as the chief. This could be based on server_def.task_index,</span><br><span class="hljs-comment"># or job_def.name, or job_def.tasks. It’s entirely up to the end user.</span><br><span class="hljs-comment"># But there can be only one chief*.</span><br>is_chief = (server_def.task_index == <span class="hljs-number">0</span>)<br>server = tf.train.Server(server_def)<br><br><span class="hljs-keyword">with</span> tf.Graph().as_default():<br>  …add operations to the graph…<br>  <span class="hljs-comment"># Create a Supervisor that uses log directory on a shared file system.</span><br>  <span class="hljs-comment"># Indicate if you are the ‘chief’</span><br>  sv = Supervisor(logdir=<span class="hljs-string">‘/shared_directory/…’</span>, is_chief=is_chief)<br>  <span class="hljs-comment"># Get a Session in a TensorFlow server on the cluster.</span><br>  <span class="hljs-keyword">with</span> sv.managed_session(server.target) <span class="hljs-keyword">as</span> sess:<br>    <span class="hljs-comment"># Use the session to train the graph.</span><br>    <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> sv.should_stop():<br>      sess.run(&lt;my_train_op&gt;)<br></code></pre><br><br><p>如果有task崩溃或重启，managed_session() 将检查是否Model被初始化。如果已经初始化，它只需要创建一个session并将其返回至正在训练的正常代码中。如果model需要被初始化，主task将对它进行重新初始化，而其他task将等待模型初始化完成。 <br><br>注意：该程序方法一样适用于单进程的work，该单进程标注自己为主要的便行</p><br><br><p>▷ <em>supervisor中master的字符串形式</em> <br><br>无论运行在本机或者集群上，都可以使用以下值设定master flag：</p><br><br><ul><br><li>定义为 ” ，要求一个进程内且没有使用RPC的session</li><br><li>定义为 ‘local’，要求一个使用基于RPC的主服务接口(“Master interface” )的session来运行tensorflow程序。更多细节可以查看 tf.train.Server.create_local_server()相关内容。</li><br><li>定义为 ‘grpc://hostname:port’，要求一个指定的RPC接口的session，同时运行内部进程的master接入远程的tensorflow workers。可用server.target返回该形式</li><br></ul><br><br><p>▷ <em>supervisor高级用法</em></p><br><br><ul><br><li>启动额外的服务 <br><br>managed_session()启动了 Checkpoint 和Summary服务。如果需要运行更多的服务，可以在managed_session()控制的模块中启动他们。</li><br></ul><br><br><br><br><pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment">#例如： 开启一个线程用于打印loss. 设置每60秒该线程运行一次，我们使用sv.loop()</span><br> …<br>  sv = Supervisor(logdir=<span class="hljs-string">‘/tmp/mydir’</span>)<br>  <span class="hljs-keyword">with</span> sv.managed_session(FLAGS.master) <span class="hljs-keyword">as</span> sess:<br>    sv.loop(<span class="hljs-number">60</span>, print_loss, (sess))<br>    <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> sv.should_stop():<br>      sess.run(my_train_op)<br></code></pre><br><br><ul><br><li>启动更少的的服务 <br><br>managed_session() 启动了 “summary” 和 “checkpoint” 线程，这些线程通过构建器或者监督器默认自动创建了summary_op 和saver操作。如果想运行自己的 summary 和checkpointing方法，关闭这些服务，通过传递None值给summary_op 和saver参数。</li><br></ul><br><br><br><br><pre class="prettyprint"><code class="language-python hljs ">在chief中每<span class="hljs-number">100</span>个step，创建summaries<br>  <span class="hljs-comment"># Create a Supervisor with no automatic summaries.</span><br>  sv = Supervisor(logdir=<span class="hljs-string">‘/tmp/mydir’</span>, is_chief=is_chief, summary_op=<span class="hljs-keyword">None</span>)<br>  <span class="hljs-comment"># As summary_op was None, managed_session() does not start the</span><br>  <span class="hljs-comment"># summary thread.</span><br>  <span class="hljs-keyword">with</span> sv.managed_session(FLAGS.master) <span class="hljs-keyword">as</span> sess:<br>    <span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> xrange(<span class="hljs-number">1000000</span>):<br>      <span class="hljs-keyword">if</span> sv.should_stop():<br>        <span class="hljs-keyword">break</span><br>      <span class="hljs-keyword">if</span> is_chief <span class="hljs-keyword">and</span> step % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>        <span class="hljs-comment"># Create the summary every 100 chief steps.</span><br>        sv.summary_computed(sess, sess.run(my_summary_op))<br>      <span class="hljs-keyword">else</span>:<br>        <span class="hljs-comment"># Train normally</span><br>        sess.run(my_train_op)<br><br></code></pre><br><br><p>▷ <em>tf.train.Supervisor.managed_session</em></p><br><br><br><br><pre class="prettyprint"><code class="language-python hljs "><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span><span class="hljs-params">()</span>:</span><br>  sv = tf.train.Supervisor(…)<br>  <span class="hljs-keyword">with</span> sv.managed_session(&lt;master&gt;) <span class="hljs-keyword">as</span> sess:<br>    <span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> xrange(..):<br>      <span class="hljs-keyword">if</span> sv.should_stop():<br>        <span class="hljs-keyword">break</span><br>      sess.run(&lt;my training op&gt;)<br>      …do other things needed at each training step…</code></pre><br><br><p>▷ <em>tf.train.SessionManager</em></p><br><br><br><br><pre class="prettyprint"><code class="language-python hljs "><span class="hljs-keyword">with</span> tf.Graph().as_default():<br>   …add operations to the graph…<br>  <span class="hljs-comment"># Create a SessionManager that will checkpoint the model in ‘/tmp/mydir’.</span><br>  sm = SessionManager()<br>  sess = sm.prepare_session(master, init_op, saver, checkpoint_dir)<br>  <span class="hljs-comment"># Use the session to train the graph.</span><br>  <span class="hljs-keyword">while</span> <span class="hljs-keyword">True</span>:<br>    sess.run(&lt;my_train_op&gt;)<br><span class="hljs-comment">#其中prepare_session()初始化和恢复一个模型参数。 </span><br><br><span class="hljs-comment">#另一个进程将等待model准备完成，代码如下</span><br><span class="hljs-keyword">with</span> tf.Graph().as_default():<br>  …add operations to the graph…<br>  <span class="hljs-comment"># Create a SessionManager that will wait for the model to become ready.</span><br>  sm = SessionManager()<br>  sess = sm.wait_for_session(master)<br>  <span class="hljs-comment"># Use the session to train the graph.</span><br>  <span class="hljs-keyword">while</span> <span class="hljs-keyword">True</span>:<br>    sess.run(&lt;my_train_op&gt;)<br><span class="hljs-comment">#wait_for_session()等待一个model被其他进程初始化</span><br></code></pre><br><br><p>▷ <em>tf.train.ClusterSpec</em> <br><br>一个tf.train.ClusterSpec表示一系列的进程，这些进程都参与分布式tensorflow的计算。每一个 tf.train.Server都在一个独有的集群中构建。 <br><br>创建一个具有两个jobs及其5个tasks的集群们需要定义从job名称列表到网络地址列表之间的映射。</p><br><br><br><br><pre class="prettyprint"><code class="language-python hljs ">cluster = tf.train.ClusterSpec({<span class="hljs-string">“worker”</span>: [<span class="hljs-string">“worker0.example.com:2222”</span>,<br>                                           <span class="hljs-string">“worker1.example.com:2222”</span>,<br>                                           <span class="hljs-string">“worker2.example.com:2222”</span>],<br>                                <span class="hljs-string">“ps”</span>: [<span class="hljs-string">“ps0.example.com:2222”</span>,<br>                                       <span class="hljs-string">“ps1.example.com:2222”</span>]})</code></pre><br><br><p>▷ <em>tf.train.replica_device_setter</em></p><br><br><br><br><pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment"># To build a cluster with two ps jobs on hosts ps0 and ps1, and 3 worker</span><br><span class="hljs-comment"># jobs on hosts worker0, worker1 and worker2.</span><br>cluster_spec = {<br>    <span class="hljs-string">“ps”</span>: [<span class="hljs-string">“ps0:2222”</span>, <span class="hljs-string">“ps1:2222”</span>],<br>    <span class="hljs-string">“worker”</span>: [<span class="hljs-string">“worker0:2222”</span>, <span class="hljs-string">“worker1:2222”</span>, <span class="hljs-string">“worker2:2222”</span>]}<br><span class="hljs-keyword">with</span> tf.device(tf.replica_device_setter(cluster=cluster_spec)):<br>  <span class="hljs-comment"># Build your graph</span><br>  v1 = tf.Variable(…)  <span class="hljs-comment"># assigned to /job:ps/task:0</span><br>  v2 = tf.Variable(…)  <span class="hljs-comment"># assigned to /job:ps/task:1</span><br>  v3 = tf.Variable(…)  <span class="hljs-comment"># assigned to /job:ps/task:0</span><br><span class="hljs-comment"># Run compute</span><br></code></pre><br><br><p><br></p><br><br><br><br><h4 id="汇总操作summary-operations">█ <strong>汇总操作(Summary Operations)</strong></h4><br><br><p>我们可以在一个session中获取summary操作的输出，并将其传输到SummaryWriter以添加至一个事件记录文件中。</p><br><br><table><br><thead><br><tr><br>  <th>操作</th><br>  <th align="left">描述</th><br></tr><br></thead><br><tbody><tr><br>  <td>tf.scalar_summary(tags, values, collections=None, name=None)</td><br>  <td align="left">输出一个标量值的summary协议buffer<br>tag的shape需要与values的相同，用来做summaries的tags，为字符串</td><br></tr><br><tr><br>  <td>tf.image_summary(tag, tensor, max_images=3, collections=None, name=None)</td><br>  <td align="left">输出一个图像tensor的summary协议buffer</td><br></tr><br><tr><br>  <td>tf.audio_summary(tag, tensor, sample_rate, max_outputs=3, collections=None, name=None)</td><br>  <td align="left">输出一个音频tensor的summary协议buffer</td><br></tr><br><tr><br>  <td>tf.histogram_summary(tag, values, collections=None, name=None)</td><br>  <td align="left">输出一个直方图的summary协议buffer</td><br></tr><br><tr><br>  <td>tf.nn.zero_fraction(value, name=None)</td><br>  <td align="left">返回0在value中的小数比例</td><br></tr><br><tr><br>  <td>tf.merge_summary(inputs, collections=None, name=None)</td><br>  <td align="left">合并summary</td><br></tr><br><tr><br>  <td>tf.merge_all_summaries(key=’summaries’)</td><br>  <td align="left">合并在默认graph中手机的summaries</td><br></tr><br></tbody></table><br><br><br><p>▶▶<strong><em>将记录汇总写入文件中(Adding Summaries to Event Files)</em></strong></p><br><br><table><br><thead><br><tr><br>  <th>操作</th><br>  <th align="left">描述</th><br></tr><br></thead><br><tbody><tr><br>  <td>class tf.train.SummaryWriter</td><br>  <td align="left">将summary协议buffer写入事件文件中</td><br></tr><br><tr><br>  <td>tf.train.SummaryWriter.<strong>init</strong>(logdir, graph=None, max_queue=10, flush_secs=120, graph_def=None)</td><br>  <td align="left">创建一个SummaryWriter实例以及新建一个事件文件</td><br></tr><br><tr><br>  <td>tf.train.SummaryWriter.add_summary(summary, global_step=None)</td><br>  <td align="left">将一个summary添加到事件文件中</td><br></tr><br><tr><br>  <td>tf.train.SummaryWriter.add_session_log(session_log, global_step=None)</td><br>  <td align="left">添加SessionLog到一个事件文件中</td><br></tr><br><tr><br>  <td>tf.train.SummaryWriter.add_event(event)</td><br>  <td align="left">添加一个事件到事件文件中</td><br></tr><br><tr><br>  <td>tf.train.SummaryWriter.add_graph(graph, global_step=None, graph_def=None)</td><br>  <td align="left">添加一个Graph到时间文件中</td><br></tr><br><tr><br>  <td>tf.train.SummaryWriter.add_run_metadata(run_metadata, tag, global_step=None)</td><br>  <td align="left">为一个单一的session.run()调用添加一个元数据信息</td><br></tr><br><tr><br>  <td>tf.train.SummaryWriter.flush()</td><br>  <td align="left">刷新时间文件到硬盘中</td><br></tr><br><tr><br>  <td>tf.train.SummaryWriter.close()</td><br>  <td align="left">将事件问价写入硬盘中并关闭该文件</td><br></tr><br><tr><br>  <td>tf.train.summary_iterator(path)</td><br>  <td align="left">一个用于从时间文件中读取时间协议buffer的迭代器</td><br></tr><br></tbody></table><br><br><br><p>▷ <em>tf.train.SummaryWriter</em> <br><br>创建一个SummaryWriter 和事件文件。如果我们传递一个Graph进入该构建器中，它将被添加到事件文件当中，这一点与使用add_graph()具有相同功能。 <br><br>TensorBoard 将从事件文件中提取该graph，并将其显示。所以我们能直观地看到我们建立的graph。我们通常从我们启动的session中传递graph：</p><br><br><br><br><pre class="prettyprint"><code class="language-python hljs "><br>…create a graph…<br><span class="hljs-comment"># Launch the graph in a session.</span><br>sess = tf.Session()<br><span class="hljs-comment"># Create a summary writer, add the ‘graph’ to the event file.</span><br>writer = tf.train.SummaryWriter(&lt;some-directory&gt;, sess.graph)<br></code></pre><br><br><p>▷ <em>tf.train.summary_iterator</em></p><br><br><br><br><pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment">#打印时间文件中的内容</span><br><span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> tf.train.summary_iterator(path to events file):<br>    print(e)<br><br><span class="hljs-comment">#打印指定的summary值</span><br><span class="hljs-comment"># This example supposes that the events file contains summaries with a</span><br><span class="hljs-comment"># summary value tag ‘loss’.  These could have been added by calling</span><br><span class="hljs-comment"># <code>add_summary()</code>, passing the output of a scalar summary op created with</span><br><span class="hljs-comment"># with: <code>tf.scalar_summary([&#39;loss&#39;], loss_tensor)</code>.</span><br><span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> tf.train.summary_iterator(path to events file):<br>    <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> e.summary.value:<br>        <span class="hljs-keyword">if</span> v.tag == <span class="hljs-string">‘loss’</span>:<br>            print(v.simple_value)</code></pre><br><br><p><br></p><br><br><br><br><h4 id="训练的通用函数及其他training-utilities">█ <strong>训练的通用函数及其他(Training utilities)</strong></h4><br><br><table><br><thead><br><tr><br>  <th>操作</th><br>  <th align="left">描述</th><br></tr><br></thead><br><tbody><tr><br>  <td>tf.train.global_step(sess, global_step_tensor)</td><br>  <td align="left">一个用于获取全局step的小辅助器</td><br></tr><br><tr><br>  <td>tf.train.write_graph(graph_def, logdir, name, as_text=True)</td><br>  <td align="left">将一个graph proto写入一个文件中</td><br></tr><br><tr><br>  <td>#</td><br>  <td align="left"></td><br></tr><br><tr><br>  <td></td><br>  <td align="left">:—</td><br></tr><br><tr><br>  <td>class tf.train.LooperThread</td><br>  <td align="left">可重复地执行代码的线程</td><br></tr><br><tr><br>  <td>tf.train.LooperThread.<strong>init</strong>(coord, timer_interval_secs, target=None, args=None, kwargs=None)</td><br>  <td align="left">创建一个LooperThread</td><br></tr><br><tr><br>  <td>tf.train.LooperThread.is_alive()</td><br>  <td align="left">返回是否该线程是活跃的</td><br></tr><br><tr><br>  <td>tf.train.LooperThread.join(timeout=None)</td><br>  <td align="left">等待线程结束</td><br></tr><br><tr><br>  <td>tf.train.LooperThread.loop(coord, timer_interval_secs, target, args=None, kwargs=None)</td><br>  <td align="left">启动一个LooperThread，用于周期地调用某个函数<br>调用函数target(args)</td><br></tr><br><tr><br>  <td>tf.py_func(func, inp, Tout, stateful=True, name=None)</td><br>  <td align="left">将python函数包装成tf中操作节点</td><br></tr><br></tbody></table><br><br><br><p>▷ <em>tf.train.global_step</em></p><br><br><br><br><pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment"># Creates a variable to hold the global_step.</span><br>global_step_tensor = tf.Variable(<span class="hljs-number">10</span>, trainable=<span class="hljs-keyword">False</span>, name=<span class="hljs-string">‘global_step’</span>)<br><span class="hljs-comment"># Creates a session.</span><br>sess = tf.Session()<br><span class="hljs-comment"># Initializes the variable.</span><br>sess.run(global_step_tensor.initializer)<br>print(<span class="hljs-string">‘global_step: %s’</span> % tf.train.global_step(sess, global_step_tensor))<br><br>global_step: <span class="hljs-number">10</span><br></code></pre><br><br><p>▷ <em>tf.train.write_graph</em></p><br><br><br><br><pre class="prettyprint"><code class="language-python hljs ">v = tf.Variable(<span class="hljs-number">0</span>, name=<span class="hljs-string">‘my_variable’</span>)<br>sess = tf.Session()<br>tf.train.write_graph(sess.graph_def, <span class="hljs-string">‘/tmp/my-model’</span>, <span class="hljs-string">‘train.pbtxt’</span>)</code></pre><br><br><p>▷ <em>tf.py_func</em></p><br><br><br><br><pre class="prettyprint"><code class="language-python hljs "><span class="hljs-comment">#tf.py_func(func, inp, Tout, stateful=True, name=None)</span><br><span class="hljs-comment">#func：为一个python函数</span><br><span class="hljs-comment">#inp：为输入函数的参数，Tensor列表</span><br><span class="hljs-comment">#Tout： 指定func返回的输出的数据类型，是一个列表</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">my_func</span><span class="hljs-params">(x)</span>:</span><br>  <span class="hljs-comment"># x will be a numpy array with the contents of the placeholder below</span><br>  <span class="hljs-keyword">return</span> np.sinh(x)<br>inp = tf.placeholder(tf.float32, […])<br>y = py_func(my_func, [inp], [tf.float32])<br></code></pre><br><br><br><br><h3 id="22-测试-testing">2.2 测试 (Testing)</h3><br><br><p>TensorFlow 提供了一个方便的继承unittest.TestCase类的方法，该类增加有关TensorFlow 测试的方法。如下例子：</p><br><br><br><br><pre class="prettyprint"><code class="language-python hljs "><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><br><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">SquareTest</span><span class="hljs-params">(tf.test.TestCase)</span>:</span><br><br>  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">testSquare</span><span class="hljs-params">(self)</span>:</span><br>    <span class="hljs-keyword">with</span> self.test_session():<br>      x = tf.square([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>])<br>      self.assertAllEqual(x.eval(), [<span class="hljs-number">4</span>, <span class="hljs-number">9</span>])<br><br><br><span class="hljs-keyword">if</span> <strong>name</strong> == <span class="hljs-string">‘<strong>main</strong>‘</span>:<br>  tf.test.main()</code></pre><br><br><p><br></p><br><br><br><br><h4 id="共用utilities">█ <strong>共用(Utilities)</strong></h4><br><br><table><br><thead><br><tr><br>  <th>操作</th><br>  <th align="left">描述</th><br></tr><br></thead><br><tbody><tr><br>  <td>tf.test.main()</td><br>  <td align="left">运行所有的单元测试</td><br></tr><br><tr><br>  <td>tf.test.assert_equal_graph_def(actual, expected)</td><br>  <td align="left">断言 两个GraphDefs 是否几乎一样</td><br></tr><br><tr><br>  <td>tf.test.get_temp_dir()</td><br>  <td align="left">返回测试期间使用的临时目录</td><br></tr><br><tr><br>  <td>tf.test.is_built_with_cuda()</td><br>  <td align="left">返回是否Tensorflow支持CUDA(GPU)的build</td><br></tr><br></tbody></table><br><br><br><p><br></p><br><br><h4 id="梯度检查gradient-checking">█ <strong>梯度检查(Gradient checking)</strong></h4><br><br><p>可对比compute_gradient 和compute_gradient_error函数的用法</p><br><br><table><br><thead><br><tr><br>  <th>操作</th><br>  <th align="left">描述</th><br></tr><br></thead><br><tbody><tr><br>  <td>tf.test.compute_gradient(x, x_shape, y, y_shape, x_init_value=None, delta=0.001, init_targets=None)</td><br>  <td align="left">计算并返回理论的和数值的Jacobian矩阵</td><br></tr><br><tr><br>  <td>tf.test.compute_gradient_error(x, x_shape, y, y_shape, x_init_value=None, delta=0.001, init_targets=None)</td><br>  <td align="left">计算梯度的error。在计算所得的与数值估计的Jacobian中 为dy/dx计算最大的error</td><br></tr><br></tbody></table><br><br><br><hr><br><br><hr><br><br><p><strong>相关链接：</strong></p><br><br><p>[1] 安装Tensorflow（Linux ubuntu） <a href="http://blog.csdn.net/lenbow/article/details/51203526" target="_blank" rel="noopener">http://blog.csdn.net/lenbow/article/details/51203526</a> <br><br>[2] ubuntu下CUDA编译的GCC降级安装 <a href="http://blog.csdn.net/lenbow/article/details/51596706" target="_blank" rel="noopener">http://blog.csdn.net/lenbow/article/details/51596706</a> <br><br>[3] ubuntu手动安装最新Nvidia显卡驱动 <a href="http://blog.csdn.net/lenbow/article/details/51683783" target="_blank" rel="noopener">http://blog.csdn.net/lenbow/article/details/51683783</a> <br><br>[4] Tensorflow的CUDA升级，以及相关配置 <a href="http://blog.csdn.net/lenbow/article/details/52118116" target="_blank" rel="noopener">http://blog.csdn.net/lenbow/article/details/52118116</a> <br><br>[5] 基于gensim的Doc2Vec简析 <a href="http://blog.csdn.net/lenbow/article/details/52120230" target="_blank" rel="noopener">http://blog.csdn.net/lenbow/article/details/52120230</a> <br><br>[6] TensorFlow的分布式学习框架简介 <a href="http://blog.csdn.net/lenbow/article/details/52130565" target="_blank" rel="noopener">http://blog.csdn.net/lenbow/article/details/52130565</a> <br><br>[7] Tensorflow一些常用基本概念与函数（1） <a href="http://blog.csdn.net/lenbow/article/details/52152766" target="_blank" rel="noopener">http://blog.csdn.net/lenbow/article/details/52152766</a> <br><br>[8] Tensorflow一些常用基本概念与函数（2） <a href="http://blog.csdn.net/lenbow/article/details/52181159" target="_blank" rel="noopener">http://blog.csdn.net/lenbow/article/details/52181159</a> <br><br>[9] Tensorflow一些常用基本概念与函数（3） <a href="http://blog.csdn.net/lenbow/article/details/52213105" target="_blank" rel="noopener">http://blog.csdn.net/lenbow/article/details/52213105</a></p></div>
</div></div></div>
    </div>

    
    
    
      
  <div class="popular-posts-header">相关文章</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/AI/IPCreator/Technology/AI/TensorFlow/basic concepts of tensorflow/" rel="bookmark">Basic Concepts of Tensorflow</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/AI/IPCreator/Technology/AI/Tensorflow and Android/" rel="bookmark">Tensorflow and Android</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/AI/IPCreator/Technology/AI/TensorFlow/Tensorflow Lite/" rel="bookmark">Tensorflow Lite</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/AI/IPCreator/Technology/AI/TensorFlow/changes-by-google-tensorflow/" rel="bookmark">Tensorflow Mean the Big Reform of Hardware</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/AI/IPCreator/Technology/AI/TensorFlow/tensorflow-android-camera-demo/" rel="bookmark">TensorFlow Android Camera Demo</a></div>
    </li>
  </ul>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/Tensorflow/" rel="tag"><i class="fa fa-tag"></i> Tensorflow</a>
          </div>

        
  <div class="post-widgets">
    <div class="wp_rating">
      <div id="wpac-rating"></div>
    </div>
  </div>


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/AI/IPCreator/Technology/AI/TensorFlow/benchmark-of-machine-learning/" rel="prev" title="Compare of AI platforms">
      <i class="fa fa-chevron-left"></i> Compare of AI platforms
    </a></div>
      <div class="post-nav-item">
    <a href="/AI/IPCreator/Technology/AI/TensorFlow/good-understanding-of-tensorflow/" rel="next" title="Deep | Everybody can Understand Tensorflow">
      Deep | Everybody can Understand Tensorflow <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#1tensorflow的基本运作"><span class="nav-number">1.</span> <span class="nav-text">1、tensorflow的基本运作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2tf函数"><span class="nav-number">2.</span> <span class="nav-text">2、tf函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#tensorflow的算术操作如下"><span class="nav-number">2.1.</span> <span class="nav-text">TensorFlow的算术操作如下：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#张量操作tensor-transformations"><span class="nav-number">2.2.</span> <span class="nav-text">张量操作Tensor Transformations</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#矩阵相关运算"><span class="nav-number">2.3.</span> <span class="nav-text">矩阵相关运算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#复数操作"><span class="nav-number">2.4.</span> <span class="nav-text">复数操作</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#归约计算reduction"><span class="nav-number">2.5.</span> <span class="nav-text">归约计算(Reduction)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#分割segmentation"><span class="nav-number">2.6.</span> <span class="nav-text">分割(Segmentation)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#序列比较与索引提取sequence-comparison-and-indexing"><span class="nav-number">2.7.</span> <span class="nav-text">序列比较与索引提取(Sequence Comparison and Indexing)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#神经网络neural-network"><span class="nav-number">2.8.</span> <span class="nav-text">神经网络(Neural Network)</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#保存与恢复变量"><span class="nav-number"></span> <span class="nav-text">保存与恢复变量</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1tensorflow的基本运作"><span class="nav-number"></span> <span class="nav-text">1、tensorflow的基本运作</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2tf函数"><span class="nav-number"></span> <span class="nav-text">2、tf函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#21-建立图building-graphs"><span class="nav-number">1.</span> <span class="nav-text">2.1 建立图(Building Graphs)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#核心图的数据结构core-graph-data-structures"><span class="nav-number">1.1.</span> <span class="nav-text"> 核心图的数据结构（Core graph data structures）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tensor类型tensor-types"><span class="nav-number">1.2.</span> <span class="nav-text"> tensor类型(Tensor types)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#通用函数utility-functions"><span class="nav-number">1.3.</span> <span class="nav-text"> 通用函数（Utility functions）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#图收集graph-collections"><span class="nav-number">1.4.</span> <span class="nav-text"> 图收集（Graph collections）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#定义新操作节点defining-new-operations"><span class="nav-number">1.5.</span> <span class="nav-text"> 定义新操作节点（Defining new operations）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#22-输入和读取器inputs-and-readers"><span class="nav-number">2.</span> <span class="nav-text">2.2 输入和读取器(Inputs and Readers)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#占位符placeholders"><span class="nav-number">2.1.</span> <span class="nav-text"> 占位符（Placeholders）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#读取器readers"><span class="nav-number">2.2.</span> <span class="nav-text"> 读取器（Readers）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#数据转换converting"><span class="nav-number">2.3.</span> <span class="nav-text"> 数据转换（Converting）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#example-protocol-buffer"><span class="nav-number">2.4.</span> <span class="nav-text"> Example protocol buffer</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#队列queues"><span class="nav-number">2.5.</span> <span class="nav-text"> 队列(Queues)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#文件系统的处理dealing-with-the-filesystem"><span class="nav-number">2.6.</span> <span class="nav-text"> 文件系统的处理(Dealing with the filesystem)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#输入管道input-pipeline"><span class="nav-number">2.7.</span> <span class="nav-text"> 输入管道(Input pipeline)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#在输入管道末端批量打包batching-at-the-end-of-an-input-pipeline"><span class="nav-number">2.8.</span> <span class="nav-text"> 在输入管道末端批量打包(Batching at the end of an input pipeline)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1序言"><span class="nav-number"></span> <span class="nav-text">1、序言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2tf函数"><span class="nav-number"></span> <span class="nav-text">2、tf函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#21-数据io-data-io-python-functions"><span class="nav-number">1.</span> <span class="nav-text">2.1 数据IO {Data IO (Python functions)}</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#数据io-data-io-python-functions"><span class="nav-number">1.1.</span> <span class="nav-text">数据IO {Data IO (Python functions)}</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#22-运行图running-graphs"><span class="nav-number">2.</span> <span class="nav-text">2.2 运行图(Running Graphs)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#会话管理-session-management"><span class="nav-number">2.1.</span> <span class="nav-text">会话管理 (Session management)</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#abc"><span class="nav-number"></span> <span class="nav-text">ABC</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#错误类-error-classes"><span class="nav-number">0.1.</span> <span class="nav-text">错误类 (Error classes)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1序言"><span class="nav-number"></span> <span class="nav-text">1、序言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2tensorflow函数"><span class="nav-number"></span> <span class="nav-text">2、Tensorflow函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#21-训练-training"><span class="nav-number">1.</span> <span class="nav-text">2.1 训练 (Training)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#优化-optimizers"><span class="nav-number">1.1.</span> <span class="nav-text">█ 优化 (Optimizers)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#slots"><span class="nav-number">1.2.</span> <span class="nav-text">█ Slots</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#梯度计算与截断gradient-computation-and-clipping"><span class="nav-number">1.3.</span> <span class="nav-text">█ 梯度计算与截断(Gradient Computation and Clipping)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#退化学习率decaying-the-learning-rate"><span class="nav-number">1.4.</span> <span class="nav-text">█ 退化学习率(Decaying the learning rate)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#移动平均moving-averages"><span class="nav-number">1.5.</span> <span class="nav-text">█ 移动平均(Moving Averages)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#协调器和队列运行器coordinator-and-queuerunner"><span class="nav-number">1.6.</span> <span class="nav-text">█ 协调器和队列运行器(Coordinator and QueueRunner)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#布执行distributed-execution"><span class="nav-number">1.7.</span> <span class="nav-text">█ 布执行(Distributed execution)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#汇总操作summary-operations"><span class="nav-number">1.8.</span> <span class="nav-text">█ 汇总操作(Summary Operations)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#训练的通用函数及其他training-utilities"><span class="nav-number">1.9.</span> <span class="nav-text">█ 训练的通用函数及其他(Training utilities)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#22-测试-testing"><span class="nav-number">2.</span> <span class="nav-text">2.2 测试 (Testing)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#共用utilities"><span class="nav-number">2.1.</span> <span class="nav-text">█ 共用(Utilities)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#梯度检查gradient-checking"><span class="nav-number">2.2.</span> <span class="nav-text">█ 梯度检查(Gradient checking)</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="IPCreator"
      src="/img/content/Kick-Off.jpg">
  <p class="site-author-name" itemprop="name">IPCreator</p>
  <div class="site-description" itemprop="description">Life is a journey.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">1559</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">1460</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/IPCreator1833" title="GitHub → https://github.com/IPCreator1833" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/atom.xml" title="RSS → /atom.xml"><i class="fa fa-rss fa-fw"></i>RSS</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://blog.163.com/zhuxuanlv@126/" title="http://blog.163.com/zhuxuanlv@126/" rel="noopener" target="_blank">163 Blog</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2017 – 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="far fa-smile"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">IPCreator</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">28.2m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">427:04</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script>
<script src="/js/schemes/muse.js"></script>
<script src="/js/next-boot.js"></script>



  



  <script>
  if (CONFIG.page.isPost) {
    wpac_init = window.wpac_init || [];
    wpac_init.push({
      widget: 'Rating',
      id    : 11184,
      el    : 'wpac-rating',
      color : 'fc6423'
    });
    (function() {
      if ('WIDGETPACK_LOADED' in window) return;
      WIDGETPACK_LOADED = true;
      var mc = document.createElement('script');
      mc.type = 'text/javascript';
      mc.async = true;
      mc.src = '//embed.widgetpack.com/widget.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(mc, s.nextSibling);
    })();
  }
  </script>

  <script src="/js/local-search.js"></script>












  

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'https://hazyman.com/AI/IPCreator/Technology/AI/TensorFlow/basic-concept-and-operation-of-tensorflow/',]
      });
      });
  </script>


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'C75jI23HEPhTehiFhCvUSbJY-gzGzoHsz',
      appKey     : 'qy3Id9srq8HBxwKg3CVSdNNq',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
